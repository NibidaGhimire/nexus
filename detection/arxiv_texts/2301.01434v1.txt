arXiv:2301.01434v1  [cs.LG]  4 Jan 2023Online Learning of Smooth Functions
Jesse Geneson and Ethan Zhou
January 5, 2023
Abstract
In this paper, we study the online learning ofreal-valuedfunctions w here the hidden function
is known to have certain smoothness properties. Speciﬁcally, for q≥1, letFqbe the class of
absolutely continuous functions f: [0,1]→Rsuch that /⌊ard⌊lf′/⌊ard⌊lq≤1. Forq≥1 andd∈Z+,
letFq,dbe the class of functions f: [0,1]d→Rsuch that any function g: [0,1]→Rformed
by ﬁxing all but one parameter of fis inFq. For any class of real-valued functions Fand
p >0, let optp(F) be the best upper bound on the sum of pthpowers of absolute prediction
errors that a learner can guarantee in the worst case. In the sing le-variable setup, we ﬁnd new
bounds for optp(Fq) that are sharp up to a constant factor. We show for all ε∈(0,1) that
opt1+ε(F∞) = Θ(ε−1
2) and opt1+ε(Fq) = Θ(ε−1
2) for allq≥2. We also show for ε∈(0,1) that
opt2(F1+ε) = Θ(ε−1). In addition, we obtain new exact results by proving that optp(Fq) = 1
forq∈(1,2) andp≥2 +1
q−1. In the multi-variable setup, we establish inequalities relating
optp(Fq,d) to optp(Fq) and show that optp(F∞,d) is inﬁnite when p < dand ﬁnite when p > d.
We also obtain sharp bounds on learning F∞,dforp < dwhen the number of trials is bounded.
1 Introduction
Consider a learner that wants to predict the next day’s tempe rature range at a given location
based on inputs such as the current day’s temperature range, humidity, atmospheric pressure,
precipitation, wind speed, solar radiation, location, and time of year. In our model, this learner
is tested daily. On a given day, the learner gets inputs for th at day, which it uses to output a
prediction for the next day’s temperature range; when the ne xt day arrives, it sees the correct
temperature range, then uses this feedback to update future predictions. As this is repeated, the
learner accumulates information to help it make better pred ictions. A natural question arises: can
the learner guarantee that its predictions become better ov er time, and if so, how quickly?
We investigate a model of online learning of real-valued fun ctions previously studied in [9,
12, 13, 1, 10, 11] where an algorithm Alearns a real-valued function ffrom some class Fin
trials. Past research on this model focused on functions of o ne input, for example, predicting
the temperature range solely based on the time of year. The re search showed that, as long as the
function is suﬃciently smooth, thelearner can become a good predictor fairly rapidly. Supposethat
Fconsists of functions f:S→Rfor some set S, and ﬁx some f∈ F. In each trial t= 0,...,m,
Areceives an input st∈S, guesses ˆ ytfor the value of f(st), and receives the actual value of f(st).
Following [9], we focus on an error function which measures h ow diﬃcult it is for a learner to
predict functions accurately in the worst case. The error fu nction depends on two parameters, p
andq, which determine how harshly the learner is punished for err ors and the types of functions
that the learner might encounter, respectively. Small valu es ofpandqare more diﬃcult for the
11 2121 O/parenleftig
1
p−1/parenrightig
pq
Figure 1: Exact values and bounds on optp(Fq) forp,q >1 prior to the results in this paper
learner, leading to higher values of the error function. For each algorithm A,p >0,f∈ F, and
σ= (s0,...,sm)∈Sm+1, deﬁne
Lp(A,f,σ) =m/summationdisplay
t=1|ˆyt−f(st)|p.
Whenfandσare clear from the context, we refer to Lp(A,f,σ) asthe total p-error of A. Deﬁne
Lp(A,F) = sup
f∈F,σ∈∪m∈Z+SmLp(A,f,σ)
and optp(F) = inf
ALp(A,F). Note that unlike the deﬁnition of opt presented in [9, 12, 5 ],Fmay
consist of real-valued functions on any domain, not just fun ctions from [0 ,1] toR.
The case where Fcontains functions f: [0,1]→Rwhose derivatives have various bounded
norms was studied in [9, 12, 5]. For q≥1, letFqbe the class of absolutely continuous functions
f: [0,1]→Rsuchthat/integraltext1
0|f′(x)|qdx≤1, andlet F∞betheclass ofabsolutely continuous functions
f: [0,1]→Rsuch that sup
x∈(0,1)|f′(x)| ≤1. As noted in [12], F∞contains exactly those f: [0,1]→R
such that |f(x)−f(y)| ≤ |x−y|for allx,y∈[0,1]. Also, by Jensen’s inequality, F∞⊆ Fq⊆ Fr
for allq≥r≥1. Hence optp(F∞)≤optp(Fq)≤optp(Fr) for allp≥1 andq≥r≥1. Previous
papers determined the exact values of optp(Fq) forp= 1,q= 1, and p,q≥2, as well as bounds on
optp(Fq) forp∈(1,2) andq≥2.
The paper [9] proved that optp(F1) =∞for allp≥1. They also showed that opt1(Fq) =
opt1(F∞) =∞for allq≥1. In contrast, they found that optp(Fq) = optp(F∞) = 1 for all p≥2
andq≥2. This was also proved in [4] using a diﬀerent algorithm based on a generalization of
the Widrow-Hoﬀ algorithm [8, 14], and a noisy version of this problem was studied in [3]. In this
paper, we extend the region of values of p,qfor which it is known that optp(Fq) = 1.
Theorem 1.1. For any reals q >1andp≥2+1
q−1, we have optp(Fq) = 1.
Forp= 1 +εwithε∈(0,1), the paper [9] proved that optp(Fq) =O(ε−1) for all q≥2,
which implies that optp(F∞) =O(ε−1). However, these bounds are not sharp. In this paper, we
determine opt1+ε(Fq) up to a constant factor for all ε∈(0,1) andq≥2.
21 2121 Θ/parenleftig
1√p−1/parenrightig
1O/parenleftig
1
q−1/parenrightig
pq
Figure 2: Exact values and bounds on optp(Fq) forp,q >1 including the results in this paper
Theorem 1.2. For allε∈(0,1), we have opt1+ε(F∞) = Θ(ε−1
2)andopt1+ε(Fq) = Θ(ε−1
2)for all
q≥2, where the constants in the bound do not depend on q.
The proof of Theorem 1.2 splits into an upper bound and a lower bound. For the upper bound,
we use H¨ older’s inequality combined with results from [9]. For the lower bound, we modify a
construction used in [12], which obtained bounds on a ﬁnite v ariant of opt1(Fq) that depends on
the number of trials m.
The results of [9] and [12] left open the problem of determini ng optp(Fq) forq∈(1,2). It
was not even known up to a constant factor. We make progress on this problem by determining
opt2(F1+ε) up to a constant factor for ε∈(0,1). Figure 1 shows the bounds and exact values
known for p,q >1 prior to the results in our paper, while Figure 2 shows the bo unds and exact
values known for p,q >1 including the results in our paper.
Theorem 1.3. Forε∈(0,1), we have opt2(F1+ε) = Θ(ε−1).
The paper [9] also discussed the problem of online learning f or smooth functions of multiple
variables. Previous research on learning multi-variable f unctions [2, 6, 7] has focused on expected
loss rather than worst-case loss, using models where the inp utsxiare determined by a probability
distribution.
Weintroduceanaturalextensionofthesingle-variableset upfrom[9]tomulti-variablefunctions.
Speciﬁcally, for q≥1 andd∈Z+, letFq,dbe the class of functions f: [0,1]d→Rsuch that for
any (d−1)-tuple ( x1,...,x d−1)∈[0,1]d−1and integer iwith 1≤i≤d, the function g: [0,1]→R
given by g(x) =f(vi,x) is inFq, wherevi,x∈[0,1]dis the vector formed when xis inserted at the
ithposition of ( x1,...,x d−1).
One of the most fundamental questions about optp(Fq,d) is to determine when it is ﬁnite and
when it is inﬁnite. We answer this question almost completel y whenq=∞.
Theorem 1.4. For any positive integer d,optp(F∞,d)is ﬁnite when p > dand inﬁnite when
0< p < d.
As a corollary, it immediately follows for 0 < p < dthat optp(Fq,d) =∞for allq≥1. Moreover,
it is easy to see that optp(F1,d) =∞for all positive integers dandp.
3Thepapers[9]and[12]alsoinvestigated worst-casemistak e boundsforonlinelearningofsmooth
functions when the number of trials is bounded. In particula r, using the same notation as in the
ﬁrst paragraph of this section, deﬁne
Lp(A,f,σ,m ) =m/summationdisplay
t=1|ˆyt−f(st)|p.
Moreover, deﬁne
Lp(A,F,m) = sup
f∈F,σ∈Sm+1Lp(A,f,σ,m )
and optp(F,m) = inf
ALp(A,F,m).
Thepaper[9]provedthatopt1(Fq,m) =O(log(m))forallq≥2andopt1(F2,m) = Ω(/radicalbig
log(m)).
The paper [12] sharpened these bounds by proving that opt1(Fq,m) = Θ(/radicalbig
log(m)) for all q≥2
and opt1(F2,m) =√
log2(m)
2±O(1). We obtain sharp bounds for online learning of smooth fun c-
tions with a bounded number of trials when 0 < p < d. In particular, these sharp bounds are also
new in the single-variable case.
Theorem 1.5. For any positive integer dand real number pwith0< p < d, we have optp(F∞,d,m) =
Θ(m1−p
d), where the constants in the bounds depend on pandd.
InSection 2, wefocuson thesingle-variable setup. Weprove Theorem1.3 inSubsections2.1 and
2.2. Subsection 2.1 establishes the lower bound, while Subs ection 2.2 establishes the upper bound
along with several useful lemmas. Subsection 2.3 focuses on proving Theorem 1.1. In Subsection
2.4, we prove Theorem 1.2. In Section 3, we focus on the multi- variable setup, establishing various
bounds on optp(Fq,d). Finally, in Section 4, we discuss open problems.
2 Results in the single-variable setup for q∈(1,2)
First, we adopt some notation from [9]. For f: [0,1]→R, deﬁne the q-actionoff, denoted by
Jq[f], as
Jq[f] =/integraldisplay1
0|f′(x)|qdx,
so thatFqis exactly the set of absolutely continuous f: [0,1]→Rsuch that Jq[f]≤1.
Also, foranonemptyset S={(ui,vi) : 1≤i≤m}ofpointsin[0 ,1]×Rsuchthat u1< ... < u m,
deﬁne
fS(x) =

v1 x≤u1
vi+(x−ui)(vi+1−vi)
ui+1−uix∈(ui,ui+1]
vm x > um
and setf∅(x)≡0.
Finally, deﬁne the learning algorithm LININT as follows: on trial 0, LININT guesses ˆ y0= 0,
and on trial i >0, with the points in S={(x0,f(x0)),...,(xi−1,f(xi−1))}having been revealed
and given xi, LININT guesses ˆ yi=fS(xi).
42.1 Lower bounds for optp(Fq)
First, for all p,q >1 we have an obvious lower bound for optp(Fq).
Proposition 2.1. Forp,q >1, we have optp(Fq)≥1.
The paper [9] proved that equality holds when p,q≥2. As we will see, equality also holds when
q∈(1,2) for suﬃciently large values of p.
Forq∈(1,2) andp >1, we also prove a lower bound for opt2(Fq), using an adversary strategy
similar to that in Theorem 8 of [9].
Theorem 2.2. Forq∈(1,2), we have optp(Fq)≥q
(p2peln2)(q−1).
Proof.Fixq∈(1,2) and an algorithm Afor learning Fq. Consider thefollowing family of adversary
strategies, depending on a parameter b∈(0,1). The adversary picks x0= 0 and reveals f(x0) = 0,
then picks x1= 1 and reveals f(x1) =±bsuch that |ˆy1−f(x1)| ≥b; without loss of generality,
suppose f(x1) =b. Then for the next k=/floorleftig
−qlog2b
q−1/floorrightig
trials, the adversary recursively picks xiand
f(xi) as follows. On trial 2 ≤i < k+2, the adversary sets lito be the greatest real x∈[0,1] such
thatf(x) = 0 has been previously revealed, and similarly sets rito be the least real x∈[0,1] such
thatf(x) =bhas been previously revealed, then sets xi=li+ri
2. Upon receiving A’s guess ˆyi, the
adversary reveals f(xi) = 0 or f(xi) =bsuch that |f(xi)−ˆyi| ≥b
2.
To see that this strategy is well-deﬁned, note that all the xiare distinct, so it suﬃces to
show that there exists a function f∈ Fqwhich is consistent with all ( xi,f(xi)). Indeed, take
f=f{(x0,f(x0)),...,(xk+1,f(xk+1))}which linearly interpolates between all points ( xi,f(xi)); thenfhas
only one segment of nonzero slope, with
Jq[f] = 2−k/parenleftbiggb
2−k/parenrightbiggq
= 2k(q−1)bq≤2−qlog2b
q−1·(q−1)bq= 1.
Thus the adversary guarantees an error of at least
optp(Fq)≥k+1/summationdisplay
i=1|f(xi)−ˆyi|p≥bp+k/parenleftbiggb
2/parenrightbiggp
≥ −bpqlog2b
2p(q−1).
Pickingb=e−1
pyields optp(Fq)≥q
(p2peln2)(q−1). /squaresolid
In particular, when p= 2 we get the following:
Corollary 2.3. Forq∈(1,2), we have
opt2(Fq)≥q
(8eln2)(q−1)>1
(8eln2)(q−1).
2.2 Bounds for opt2(Fq)
The main result of this section is that for ε∈(0,1), opt2(F1+ε) = Θ(ε−1). Forq∈(1,2), Corollary
2.3 gives a lower bound for opt2(Fq); we now prove an upper bound for opt2(Fq) and use this to
derive the desired result. First, we show that a similar fact to Lemma 9 in [9] holds.
Lemma 2.4. Letu1< ... < u mbe reals in [0,1]andv1,...,vmbe reals, and deﬁne S=
{(u1,v1),...,(um,vm)}. Then for any q∈(1,2)and absolutely continuous f: [0,1]→Rsuch
thatf(ui) =vifor1≤i≤m, we have Jq[f]≥Jq[fS].
5Proof.Ifm= 1, then Jq[fS] = 0 and the result is clear. Otherwise, ﬁx some absolutely co ntinuous
f: [0,1]→Rwhich is consistent with the ( ui,vi), and ﬁx 1 ≤i < m. Then by Jensen’s inequality,
/integraltextui+1
ui|f′(x)|qdx
ui+1−ui≥/parenleftigg/integraltextui+1
ui|f′(x)|dx
ui+1−ui/parenrightiggq
≥
/vextendsingle/vextendsingle/vextendsingle/integraltextui+1
uif′(x)dx/vextendsingle/vextendsingle/vextendsingle
ui+1−ui
q
=/vextendsingle/vextendsingle/vextendsingle/vextendsinglevi+1−vi
ui+1−ui/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
.
Thus we obtain
Jq[f]≥/integraldisplayum
u1|f′(x)|qdx≥m−1/summationdisplay
i=1(ui+1−ui)/vextendsingle/vextendsingle/vextendsingle/vextendsinglevi+1−vi
ui+1−ui/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
=Jq[fS]
by summing over all 1 ≤i < m. /squaresolid
This leads to the following useful fact.
Lemma 2.5. For any q >1, target function f∈ Fq, integer m≥1, and sequence of inputs
x0,...,x m∈[0,1],LININT never produces an error |ˆyi−f(xi)|>1on any trial i≥1.
Proof.Suppose otherwise, so that LININT produces an error |ˆyi−f(xi)|>1 fori≥1; then there
exists 0≤j < isuch that |f(xi)−f(xj)|>1. Letting S={(x0,f(x0)),...,(xi,f(xi))}, by Lemma
2.4
Jq[f]≥Jq[fS]≥ |xi−xj|/vextendsingle/vextendsingle/vextendsingle/vextendsinglef(xi)−f(xj)
xi−xj/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
≥ |f(xi)−f(xj)|>1,
upon which f/ne}ationslash∈ Fq, contradiction. /squaresolid
Corollary 2.6. For any q >1andp′> p >1, we have Lp′(LININT ,Fq)≤Lp(LININT ,Fq).
Proof.Oneverytrial i≥1, LININTproducesanerror |ˆyi−f(xi)| ≤1, so|ˆyi−f(xi)|p′≤ |ˆyi−f(xi)|p
for alli≥1. /squaresolid
With this, the proof proceeds similarly to the proof of Theor em 11 in [9]. Speciﬁcally, we will
compare changes in Jq[fS] as new points are added to Sto the squared errors (ˆ y−fS(x))2produced
by LININT to bound L2(LININT ,Fq). This requires the following inequalities.
Lemma 2.7. For reals a >0,b >0,q∈(1,2), andx∈(−a,b), we have
a/parenleftig
1+x
a/parenrightigq
+b/parenleftig
1−x
b/parenrightigq
−(a+b)≥2q(q−1)
a+b·x2.
Proof.Fixa,b,q, and deﬁne the function
f(x) =a/parenleftig
1+x
a/parenrightigq
+b/parenleftig
1−x
b/parenrightigq
−(a+b)−2q(q−1)
a+b·x2
forx∈(−a,b), so that we wish to show f(x)≥0 for all x∈(−a,b). Compute
f′(x) =q/bracketleftbigg/parenleftig
1+x
a/parenrightigq−1
−/parenleftig
1−x
b/parenrightigq−1
−4(q−1)
a+bx/bracketrightbigg
f′′(x) =q(q−1)/bracketleftbigg1
a/parenleftig
1+x
a/parenrightigq−2
+1
b/parenleftig
1−x
b/parenrightigq−2
−4
a+b/bracketrightbigg
f(3)(x) =q(q−1)(q−2)/bracketleftbigg1
a2/parenleftig
1+x
a/parenrightigq−3
−1
b2/parenleftig
1−x
b/parenrightigq−3/bracketrightbigg
f(4)(x) =q(q−1)(q−2)(q−3)/bracketleftbigg1
a3/parenleftig
1+x
a/parenrightigq−4
+1
b3/parenleftig
1−x
b/parenrightigq−4/bracketrightbigg
.
6First, we show f′′(x)≥0 for all x∈(−a,b). Note that f(4)(x)>0 for all xand
lim
x→−a+f′′(x) = lim
x→b−f′′(x) =∞,
sof(3)(x) is increasing on ( −a,b) and it suﬃces to check that f′′(x)≥0 at the point where
f(3)(x) = 0. Solving for this xyields
f(3)(x) = 0⇐⇒1
a2/parenleftig
1+x
a/parenrightigq−3
=1
b2/parenleftig
1−x
b/parenrightigq−3
⇐⇒/parenleftbigg1+x
a
1−x
b/parenrightbigg3−q
=b2
a2⇐⇒1+x
a
1−x
b=b2
3−q
a2
3−q
⇐⇒x=−a2
3−q+b2
3−q
aq−1
3−q+bq−1
3−q.
At thisx,
f′′(x) =q(q−1)
1
a/parenleftigg
1+−a2
3−q+b2
3−q
a2
3−q+abq−1
3−q/parenrightiggq−2
+1
b/parenleftigg
1−−a2
3−q+b2
3−q
aq−1
3−qb+b2
3−q/parenrightiggq−2
−4
a+b

=q(q−1)
1
a
(a+b)bq−1
3−q
a/parenleftig
aq−1
3−q+bq−1
3−q/parenrightig
q−2
+1
b
(a+b)aq−1
3−q
b/parenleftig
aq−1
3−q+bq−1
3−q/parenrightig
q−2
−4
a+b

=q(q−1)
/parenleftig
aq−1
3−q+bq−1
3−q/parenrightig2−q/parenleftbigg
a1−qb(q−1)(q−2)
3−q+a(q−1)(q−2)
3−qb1−q/parenrightbigg
(a+b)2−q−4
a+b

≥q(q−1)
22−q(ab)(q−1)(2−q)
2(3−q)·2(ab)(q−1)(2q−5)
2(3−q)
(a+b)2−q−4
a+b

= 4q(q−1)/bracketleftigg
1
(a+b)2−q(4ab)q−1
2−1
a+b/bracketrightigg
≥4q(q−1)/bracketleftbigg1
(a+b)2−q(a+b)q−1−1
a+b/bracketrightbigg
= 0,
where all inequalities follow from the inequality ( u+v)2≥4uv⇐⇒(u−v)2≥0 for all reals u,v.
Sincef′′(x) is minimized here, it follows that f′′(x)≥0 for all x∈(−a,b).
Sincef′(0) = 0 and f′′(x)≥0 for all x∈(−a,b), it follows that f′(x)≤0 forx <0 and
f′(x)≥0 forx >0, sof(x)≥f(0) = 0 for all x∈(−a,b). /squaresolid
Lemma 2.8. For reals a,b∈(0,1),q∈(1,2), andx/ne}ationslash∈(−a,b), we have
a/vextendsingle/vextendsingle/vextendsinglex
a+1/vextendsingle/vextendsingle/vextendsingleq
+b/vextendsingle/vextendsingle/vextendsinglex
b−1/vextendsingle/vextendsingle/vextendsingleq
−(a+b)≥(q−1)|x|q
(a+b)q−1.
Proof.Fixa,b,q; by symmetry, it suﬃces to consider x≥b. Deﬁne the function
f(x) =a/parenleftigx
a+1/parenrightigq
+b/parenleftigx
b−1/parenrightigq
−(a+b)−(q−1)xq
(a+b)q−1
7forx≥b, so that we wish to show f(x)≥0 for all x≥b. Since
f′(x) =q/parenleftigx
a+1/parenrightigq−1
+q/parenleftigx
b−1/parenrightigq−1
−q(q−1)xq−1
(a+b)q−1
≥q/parenleftbiggx
a+b/parenrightbiggq−1
+q/parenleftigx
b−1/parenrightigq−1
−qxq−1
(a+b)q−1>0
for allx > b,fis increasing, so it suﬃces to show
f(b) =a/parenleftbigga+b
a/parenrightbiggq
−(a+b)−(q−1)bq
(a+b)q−1≥0.
Dividing by a+band substituting r=a
a+b, we see that this is equivalent to
g(r) =1
rq−1−1−(q−1)(1−r)q≥0
forr∈(0,1). Asg(1)>0, it suﬃces for
g′(r) =r−q(q−1)/parenleftbig
qrq(1−r)q−1−1/parenrightbig
<0
⇐⇒qrq−1(1−r)q−1<1
r.
Forr∈(0,1), the quantity rq−1(1−r)q−1is maximized when r=1
2, so it suﬃces to prove that
q/parenleftbig1
2/parenrightbig2q−2<1 forq∈(1,2). Deﬁne h(q) =q/parenleftbig1
2/parenrightbig2q−2, soh′(q) = 41−q(1−qln4)<0 forq∈(1,2).
Sinceh(1) = 1, we have h(q)<1 forq∈(1,2). /squaresolid
Corollary 2.9. For reals a,b∈(0,1)such that a+b≤1,q∈(1,2), andx/ne}ationslash∈(−a,b), we have
a/vextendsingle/vextendsingle/vextendsinglex
a+1/vextendsingle/vextendsingle/vextendsingleq
+b/vextendsingle/vextendsingle/vextendsinglex
b−1/vextendsingle/vextendsingle/vextendsingleq
−(a+b)≥(q−1)|x|q.
Combining the above yields the following key result.
Lemma 2.10. Fixq∈(1,2), a nonempty set S={(u1,v1),...,(uk,vk)}of points in [0,1]×R,
and(x,y)∈[0,1]×Rsuch that u1< ... < u k,x/ne}ationslash=uifor any1≤i≤k, andJq/bracketleftbig
fS∪{(x,y)}/bracketrightbig
≤1.
Then
Jq/bracketleftbig
fS∪{(x,y)}/bracketrightbig
−Jq[fS]≥(q−1)(y−fS(x))2.
Proof.First, suppose x < u1. Then compared to fS, the function fS∪{(x,y)}contains a new line
segment of (possibly) nonzero slope between ( x,y) and (u1,v1), so as|y−fS(x)|=|v1−y| ≤1 (by
Lemma 2.5) and |u1−x| ≤1,
Jq/bracketleftbig
fS∪{(x,y)}/bracketrightbig
−Jq[fS] = (u1−x)/vextendsingle/vextendsingle/vextendsingle/vextendsinglev1−y
u1−x/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
≥(v1−y)2= (y−fS(x))2≥(q−1)(y−fS(x))2.
The case x > ukis similar.
Now suppose there exists an integer 1 ≤i < ksuch that ui< x < u i+1. In this case,
Jq/bracketleftbig
fS∪{(x,y)}/bracketrightbig
−Jq[fS] = (x−ui)/vextendsingle/vextendsingle/vextendsingle/vextendsingley−vi
x−ui/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
+(ui+1−x)/vextendsingle/vextendsingle/vextendsingle/vextendsinglevi+1−y
ui+1−x/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
−(ui+1−ui)/vextendsingle/vextendsingle/vextendsingle/vextendsinglevi+1−vi
ui+1−ui/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
.
8Substituting a=x−ui,b=ui+1−x,d=y−fS(x), andm=vi+1−vi
ui+1−ui=fS(x)−vi
a=vi+1−fS(x)
b, we
can rewrite the above as
Jq/bracketleftbig
fS∪{(x,y)}/bracketrightbig
−Jq[fS] =a/vextendsingle/vextendsingle/vextendsingle/vextendsinglem+d
a/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
+b/vextendsingle/vextendsingle/vextendsingle/vextendsinglem−d
b/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
−(a+b)|m|q
=|m|q/parenleftbigg
a/vextendsingle/vextendsingle/vextendsingle/vextendsingle1+d
ma/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
+b/vextendsingle/vextendsingle/vextendsingle/vextendsingle1−d
mb/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
−(a+b)/parenrightbigg
.
Then applying either Lemma 2.7 or Corollary 2.9 (depending o n whetherd
m∈(−a,b)) yields
Jq/bracketleftbig
fS∪{(x,y)}/bracketrightbig
−Jq[fS]≥ |m|qmin/braceleftigg
2q(q−1)
a+b·/parenleftbiggd
m/parenrightbigg2
,(q−1)/vextendsingle/vextendsingle/vextendsingle/vextendsingled
m/vextendsingle/vextendsingle/vextendsingle/vextendsingleq/bracerightigg
= min/braceleftbigg2q(q−1)
|m|2−q(a+b)·d2,(q−1)|d|q/bracerightbigg
.
If 0<|m| ≤1, thena+b=ui+1−ui≤1 =⇒ |m|2−q(a+b)≤1, while if |m| ≥1, then
|m|2−q(a+b)≤ |m|q(a+b)≤Jq[fS]≤Jq[fS∪{(x,y)}]≤1
by Lemma 2.4, so in either case
2q(q−1)
|m|2−q(a+b)·d2≥2q(q−1)d2≥(q−1)d2.
Moreover, since Jq/bracketleftbig
fS∪{(x,y)}/bracketrightbig
≤1, by Lemma 2.5 |d| ≤1.Hence (q−1)|d|q≥(q−1)d2as well. /squaresolid
This directly yields the desired upper bound.
Theorem 2.11. Forq∈(1,2), we have L2(LININT ,Fq)≤1
q−1.
Proof.Fix a target function f∈ Fq, an integer m≥1, and a sequence of inputs σ= (x0,...,x m)∈
[0,1]m+1. Assume without loss of generality that all xiare distinct. For 0 ≤i≤m, deﬁne
Si={(x0,f(x0)),...,(xi,f(xi))}, and suppose LININT produces guesses ˆ y0,...,ˆym∈R. By
Lemma 2.4 and Lemma 2.10,
1≥Jq[f]≥Jq[fSm] =m/summationdisplay
i=1/parenleftbig
Jq[fSi]−Jq/bracketleftbig
fSi−1/bracketrightbig/parenrightbig
≥(q−1)m/summationdisplay
i=1(ˆyi−f(xi))2,
so
L2(LININT ,f,σ) =m/summationdisplay
i=1(ˆyi−f(xi))2≤1
q−1
for anyf∈ Fq, integer m≥1, andσ∈[0,1]m+1. Thus L2(LININT ,Fq)≤1
q−1. /squaresolid
Finally, combining the above with the lower bound in Corolla ry 2.3, we get the following result.
Theorem 2.12. Forε∈(0,1), we have opt2(F1+ε) = Θ(ε−1).
Proof.Combining Corollary 2.3 and Theorem 2.11,
ε−1
8eln2<1+ε
(8eln2)ε≤opt2(F1+ε)≤ε−1.
Hence, opt2(F1+ε) = Θ(ε−1). /squaresolid
9It is simple to generalize the upper bound in Theorem 2.11 to a llp≥2.
Corollary 2.13. Forε∈(0,1)andp≥2, we have optp(F1+ε) =O(ε−1).
Proof.By Lemma 2.6, optp(F1+ε)≤Lp(LININT ,F1+ε)≤L2(LININT ,F1+ε) =O(ε−1). /squaresolid
2.3 An exact result for large p
In this section, we prove that for q∈(1,2) andp≥2+1
q−1, optp(Fq) = 1. This ﬁrst requires the
following lemma.
Lemma 2.14. For reals q∈(1,2),a∈(0,1), andu,vsatisfying |u−v| ≥(q−1)q−1
a(1−a), we have
a|u|q+(1−a)|v|q>1.
Proof.Without loss of generality, suppose u > v, so that u≥v+(q−1)q−1
a(1−a).
First, suppose v <0< u; then|u|+|v| ≥(q−1)q−1
a(1−a). By the weighted power mean inequality,
|u|(a|u|q−1)+|v|((1−a)|v|q−1)
|u|+|v|≥/parenleftigg
a−1
q−1+(1−a)−1
q−1
|u|+|v|/parenrightigg−(q−1)
=⇒a|u|q+(1−a)|v|q≥(|u|+|v|)q
/parenleftig
a−1
q−1+(1−a)−1
q−1/parenrightigq−1
≥(q−1)q(q−1)
aq(1−a)q/parenleftig
a−1
q−1+(1−a)−1
q−1/parenrightigq−1
≥(q−1)q(q−1)
2q−1aq(1−a)qmax{a−1,(1−a)−1}
=(q−1)q(q−1)
2q−1max{aq−1(1−a)q,aq(1−a)q−1}.
By the weighted arithmetic mean - geometric mean inequality , forr∈(0,1) we have
rq(1−r)q−1=qq
(q−1)q/parenleftbigg(q−1)r
q/parenrightbiggq
(1−r)q−1
≤qq
(q−1)q
q·(q−1)r
q+(q−1)(1−r)
(q−1)+q
(q−1)+q
=qq
(q−1)q/parenleftbiggq−1
2q−1/parenrightbigg2q−1
.
Thus
max/braceleftbig
aq−1(1−a)q,aq(1−a)q−1/bracerightbig
≤qq
(q−1)q/parenleftbiggq−1
2q−1/parenrightbigg2q−1
=qq(q−1)q−1
(2q−1)2q−1,
so
a|u|q+(1−a)|v|q≥(q−1)(q−1)2(2q−1)2q−1
2q−1qq.
10Consider
f(q) = (q−1)2ln(q−1)+(2q−1)ln(2q−1)−(q−1)ln2−qlnq
overq∈(1,2). Note that
f′(q) = (2(q−1)ln(q−1)+(q−1))+(2ln(2 q−1)+2)−ln2−(lnq+1)
= 1−ln2+2(q−1)ln(q−1)+(q−1−lnq)+2ln(2 q−1)
≥1−ln2+2(q−1)ln(q−1)+2ln(2 q−1),
asex≥1 +ximplies that x≥ln(1 +x) forx >−1. Since xlnxis decreasing on/parenleftbig
0,1
e/parenrightbig
and
increasing on/parenleftbig1
e,∞/parenrightbig
(so in particular xlnx≥ −1
eforx >0),
q∈(1,1.004] =⇒f′(q)≥1−ln2+0.008ln0.004>0
q∈[1.004,1.055] =⇒f′(q)≥1−ln2+0.11ln0.055+2ln1 .008>0
q∈[1.055,1.12] =⇒f′(q)≥1−ln2+0.24ln0.12+2ln1 .11>0
q∈[1.12,2) =⇒f′(q)≥1−ln2−2
e+2ln1.24>0,
so for all q∈(1,2),f′(q)>0. As lim
q→1+f(q) = 0, it follows that f(q)>0 forq∈(1,2), so
a|u|q+(1−a)|v|q≥ef(q)>1 whenever v <0< u.
Now suppose v≥0. As|x|qis increasing for x≥0,
a|u|q+(1−a)|v|q≥a/parenleftbigg(q−1)q−1
a(1−a)/parenrightbiggq
=(q−1)q(q−1)
aq−1(1−a)q.
Using the work above,
(q−1)q(q−1)
aq−1(1−a)q≥(q−1)q(q−1)
max{aq−1(1−a)q,aq(1−a)q−1}>2q−1>1,
so the inequality holds whenever v≥0. The case u≤0 is identical, which completes the proof. /squaresolid
With this, we have the following key result.
Lemma 2.15. Fixq∈(1,2), a nonempty set S={(u1,v1),...,(uk,vk)}of points in [0,1]×R,
and(x,y)∈[0,1]×Rsuch that u1< ... < u k,x/ne}ationslash=uifor any1≤i≤k, andJq/bracketleftbig
fS∪{(x,y)}/bracketrightbig
≤1.
Letp= 2+1
q−1. Then
Jq/bracketleftbig
fS∪{(x,y)}/bracketrightbig
−Jq[fS]≥ |y−fS(x)|p.
Proof.We ﬁrst show that |y−fS(x)|>(q−1)q−1andx∈(u1,uk) cannot both hold. Suppose
otherwise, so that there exists an integer 1 ≤i < ksuch that ui< x < u i+1. We will derive a
contradiction by showing Jq/bracketleftbig
fS∪{(x,y)}/bracketrightbig
>1. Clearly
Jq/bracketleftbig
fS∪{(x,y)}/bracketrightbig
≥(x−ui)/vextendsingle/vextendsingle/vextendsingle/vextendsingley−vi
x−ui/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
+(ui+1−x)/vextendsingle/vextendsingle/vextendsingle/vextendsinglevi+1−y
ui+1−x/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
.
Substituting a=x−ui,b=ui+1−x,d=y−fS(x), andm=vi+1−vi
ui+1−uias in Lemma 2.10, this
rewrites as
Jq/bracketleftbig
fS∪{(x,y)}/bracketrightbig
≥a/vextendsingle/vextendsingle/vextendsingle/vextendsinglem+d
a/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
+b/vextendsingle/vextendsingle/vextendsingle/vextendsinglem−d
b/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
.
11Asa+b=ui+1−ui≤1 andq >1,
Jq/bracketleftbig
fS∪{(x,y)}/bracketrightbig
≥a
a+b/vextendsingle/vextendsingle/vextendsingle/vextendsingle(a+b)m+d(a+b)
a/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
+b
a+b/vextendsingle/vextendsingle/vextendsingle/vextendsingle(a+b)m−d(a+b)
b/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
,
and because
|d|>(q−1)q−1=⇒/vextendsingle/vextendsingle/vextendsingle/vextendsingled(a+b)/parenleftbigg1
a+1
b/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle=|d|(a+b)2
ab≥(q−1)q−1
a
a+b·b
a+b,
applying Lemma 2.14 yields Jq/bracketleftbig
fS∪{(x,y)}/bracketrightbig
>1, contradiction.
Thus at least one of |y−fS(x)| ≤(q−1)q−1andx/ne}ationslash∈(u1,uk) holds. If
|y−fS(x)| ≤(q−1)q−1=⇒(q−1)(y−fS(x))2≥ |y−fS(x)|p,
the result follows from Lemma 2.10. Otherwise, assume witho ut loss of generality that x < u1(the
casex > ukis similar); then
Jq/bracketleftbig
fS∪{(x,y)}/bracketrightbig
−Jq[fS] = (u1−x)/vextendsingle/vextendsingle/vextendsingle/vextendsinglev1−y
u1−x/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
≥ |v1−y|q≥ |v1−y|p=|y−fS(x)|p
by Lemma 2.5 (as q <2< p) and the result holds in this case as well. /squaresolid
This immediately yields the following.
Theorem 2.16. For any reals q >1andp≥2+1
q−1, we have optp(Fq) = 1.
Proof.By Proposition 2.1 and Corollary 2.6, it suﬃces to prove that forq∈(1,2) andp= 2+1
q−1,
Lp(LININT ,Fq)≤1. Fixp= 2+1
q−1, a target function f∈ Fq, an integer m≥1, and a sequence
of inputs σ= (x0,...,x m)∈[0,1]m+1. Assume without loss of generality that all xiare distinct.
For 0≤i≤m, deﬁneSi={(x0,f(x0)),...,(xi,f(xi))}, and suppose LININT produces guesses
ˆy0,...,ˆym∈R. By Lemma 2.4 and Lemma 2.15,
1≥Jq[f]≥Jq[fSm] =m/summationdisplay
i=1/parenleftbig
Jq[fSi]−Jq/bracketleftbig
fSi−1/bracketrightbig/parenrightbig
≥m/summationdisplay
i=1|ˆyi−f(xi)|p,
soLp(LININT ,f,σ)≤1 for any f∈ Fqandσ. Thus Lp(LININT ,Fq)≤1. /squaresolid
2.4 Sharp bounds for p∈(1,2)
The paper [9] showed that opt1+ε(Fq) =O(ε−1) for all ε∈(0,1) andq≥2. In this section, we
ﬁrst improve their upper bound by proving that opt1+ε(Fq) =O(ε−1
2) for allε∈(0,1) andq≥2.
Then we show that this bound is sharp by proving that opt1+ε(Fq) = Ω(ε−1
2) for all q≥1. In
order to prove the upper bound, we use two lemmas from [9]. To s tate the lemmas and prove our
upper bound, we use the following notation. Let x0,...,x mbe any sequence of distinct elements
of [0,1], and let f∈ F2. Let ˆy1,...,ˆymbe LININT’s predictions on trials 1 ,...,m. For each i >1,
letdi= minj<i|xj−xi|and letei=|ˆyi−f(xi)|.
Lemma 2.17 ([9]).For all positive integers m, we have/summationtextm
i=1e2
i
di≤1.
12Lemma 2.18 ([9]).For all positive integers mand real numbers x >1, we have/summationtextm
i=1dx
i≤1+1
2x−2.
By combining Lemmas 2.17 and 2.18 with H¨ older’s inequality , we obtain the following sharp
upper bound.
Theorem 2.19. Ifp= 1+ε∈(1,2), thenoptp(F2) =O(ε−1
2).
Proof.First, note that
m/summationdisplay
i=1ep
i=m/summationdisplay
i=1ep
i
dp
2
i·dp
2
i.
By H¨ older’s inequality, we have
m/summationdisplay
i=1ep
i
dp
2
i·dp
2
i≤/parenleftiggm/summationdisplay
i=1e2
i
di/parenrightiggp
2/parenleftiggm/summationdisplay
i=1dp
2−p
i/parenrightigg1−p
2
.
Note that/summationtextm
i=1e2
i
di≤1 by Lemma 2.17 and
m/summationdisplay
i=1dp
2−p
i≤1+1
2p
2−p−2
by Lemma 2.18, since p >1 implies thatp
2−p>1. Thus
/parenleftiggm/summationdisplay
i=1dp
2−p
i/parenrightigg1−p
2
≤/parenleftbigg
1+1
2p
2−p−2/parenrightbigg1−p
2
.
Letδ=p
2−p−1, and note that1
δ=2−p
2p−2. Thus
/parenleftbigg
1+1
2p
2−p−2/parenrightbigg1−p
2
=/parenleftbigg
1+1
21+δ−2/parenrightbigg1−p
2
=O/parenleftigg/parenleftbigg
1+1
δ/parenrightbigg2−p
2/parenrightigg
=O/parenleftigg/parenleftbiggp
2p−2/parenrightbigg2−p
2/parenrightigg
,
where the upper bound follows from the fact that eδln2≥1+δln2. Thus we have proved that
m/summationdisplay
i=1ep
i=O/parenleftigg/parenleftbiggp
2p−2/parenrightbigg2−p
2/parenrightigg
,
so optp(F2) =O/parenleftig
(2p−2)−2−p
2/parenrightig
, where we use the fact that p2−p
2= Θ(1) for p∈(1,2) to obtain
the last bound. Since p= 1+ε, we have
optp(F2) =O/parenleftig
(2p−2)−2−p
2/parenrightig
=O/parenleftig
ε−1−ε
2/parenrightig
=O(ε−1
2),
where we use the fact that εε= Θ(1) for ε∈(0,1) to obtain the last bound. /squaresolid
We obtain the next corollary since optp(F∞)≤optp(Fr)≤optp(Fq) whenever 1 ≤q≤r.
Corollary 2.20. Ifε∈(0,1), thenopt1+ε(F∞) =O(ε−1
2)andopt1+ε(Fq) =O(ε−1
2)for allq≥2,
where the constant does not depend on q.
13In order to show that the last corollary is sharp up to a consta nt factor, we construct a family
of functions in F∞. Our proof uses the following lemma from [9] which was also us ed in [12].
Lemma 2.21 ([9]).LetS⊆[0,1]×RwithS={(ui,vi) : 1≤i≤m}andu1< u2<···< um. If
(x,y)∈[0,1]×Rand there exists 1≤j≤msuch that |x−uj|=|x−uj+1|= min i|x−ui|, then
J2[fS∪{(x,y)}] =J2[fS]+2(y−fS(x))2
mini|x−ui|.
The method in the following proof is similar to one used in [12 ] to obtain bounds for a ﬁnite
variant of opt1(Fq) forq≥2 that depends on the number of trials m.
Theorem 2.22. Ifε∈(0,1), thenopt1+ε(F∞) = Ω(ε−1
2).
Proof.Since opt1+ε(F∞)≥1 for allε∈(0,1), it suﬃces to prove the theorem for ε∈/parenleftbig
0,1
2/parenrightbig
. Deﬁne
x0= 1 and y0= 0. For natural numbers i,jwith 0≤j <2i−1, deﬁnex2i−1+j=1
2i+j
2i−1. For
eachi= 1,2,..., we consider the trials for x2i−1,...,x2i−1to be part of stage i, so that x1=1
2is
in stage 1, x2=1
4andx3=3
4are in stage 2, and so on.
LetAbeany algorithm for learning F∞. UsingA, we construct an inﬁnite sequence of piecewise
linear functions f0,f1,...∈ F∞and an inﬁnite sequence of numbers y0,y1,...∈Rfor which ftis
consistent with the xkandykvalues for k≤tandAhas total (1+ ε)-error at least
i/summationdisplay
k=12k−2/parenleftigg√ε(1−ε)k
2
2k+1/parenrightigg1+ε
afteristages. This implies that
opt1+ε(F∞)≥∞/summationdisplay
k=12k−2/parenleftigg√ε(1−ε)k
2
2k+1/parenrightigg1+ε
.
In order to analyze the functions fi, we will also deﬁne and analyze another inﬁnite sequence
of piecewise linear functions gi,jwith 0≤j≤2i−1and another inﬁnite sequence of numbers
v1,v2,...∈R. We start by letting f0be the 0-function. Next, we inductively deﬁne both sequence s
of piecewise linear functions.
Fix a stage i, and let gi,0=f2i−1−1. Lettbe a trial in stage i, and let vtbe whichever of
ft−1(xt)±√ε(1−ε)i
2
2i+1is furthest from ˆ yt. Letgi,t−2i−1+1be the function which linearly interpolates
{(0,0),(1,0)} ∪/braceleftbig
(xs,ys) :s <2i−1/bracerightbig
∪/braceleftbig
(xs,vs) : 2i−1≤s≤t/bracerightbig
.
For anyt≥1, letLtandRtbe the elements of {0,1}∪{xs:s < t}that are closest to xton the
left and right respectively. If both |vt−ft−1(Lt)| ≤2−iand|vt−ft−1(Rt)| ≤2−i, then let yt=vt.
Otherwise we let yt=ft−1(xt). Finally, we deﬁne ftto be the function which linearly interpolates
{(0,0),(1,0)} ∪{(xs,ys) :s≤t}.
By deﬁnition, we have ft∈ F∞for each t≥0. We will prove next that for all i,jwe have
J2[gi,j]≤1
4, and then we will use this to prove that yt=ft−1(xt) for at most half of the trials t
in stage i. The proof will use double induction, ﬁrst on iand then on j, and we will prove the
stronger statement that
J2[gi,j]≤ε
4i−1/summationdisplay
k=0(1−ε)k+jε(1−ε)i
2i+1. (1)
14In order to prove this statement, we will also prove that
J2[f2i−1−1]≤ε
4i−1/summationdisplay
k=0(1−ε)k(2)
for alli≥1. Note that this is equivalent to proving that
J2[gi,0]≤ε
4i−1/summationdisplay
k=0(1−ε)k
for alli≥1. Clearly this is true for i= 1, which is the base case of the induction on i. Fix some
stagei≥1. We will assume that Inequality 2 is true for this ﬁxed i, and use this to prove that
J2[f2i−1]≤ε
4i/summationdisplay
k=0(1−ε)k. (3)
In order to prove Inequality 3, we will prove Inequality 1 for all 0≤j≤2i−1. This follows from
the inductive hypothesis for iand the deﬁnition of gi,0whenj= 0, which is the base case of the
induction on j. Fix some integer jwith 0≤j≤2i−1−1 and assume that Inequality 1 is true for
this ﬁxed j. By Lemma 2.21, we have
J2[gi,j+1] =J2[gi,j]+2/parenleftbigg√ε(1−ε)i
2
2i+1/parenrightbigg2
2−i=J2[gi,j]+ε(1−ε)i
2i+1.
By the inductive hypothesis for j, we obtain
J2[gi,j+1]≤ε
4i−1/summationdisplay
k=0(1−ε)k+jε(1−ε)i
2i+1+ε(1−ε)i
2i+1,
which completes the inductive step for j. Substituting j= 2i−1, we obtain
J2[gi,2i−1]≤ε
4i/summationdisplay
k=0(1−ε)k.
Note that Lemma 2.21 implies that
J2[f2i−1−1+j]≤J2[gi,j]
for allj= 0,...,2i−1, so we obtain Inequality 3, which completes the inductive st ep fori. By
Inequality 1, we obtain
J2[gi,j]≤ε
4i−1/summationdisplay
k=0(1−ε)k+ε(1−ε)i
4=ε
4i/summationdisplay
k=0(1−ε)k
for alljwith 0≤j≤2i−1. Note that
ε
4i/summationdisplay
k=0(1−ε)k<ε
4∞/summationdisplay
k=0(1−ε)k=1
4.
15Now that we have shown that J2[gi,j]≤1
4, we are ready to prove for each i≥1 thatyt=ft−1(xt)
for at most half of the trials tin stagei. For each trial twithyt=ft−1(xt), note that the absolute
value of the slope of gi,t−2i−1+1must exceed 1 in at least one of the intervals of length 2−ion either
side ofxt. Ifyt=ft−1(xt) for at least bof the trials in stage i, then restricting to intervals of slope
at least 1 implies that J2[gi,2i−1]≥b2−i. SinceJ2[gi,2i−1]≤1
4, we must have b≤2i−2. Thus during
stagei, there are at most 2i−2trialstwithyt=ft−1(xt), which implies that there are at least 2i−2
trials with yt=vt. In each of those trials, Awas oﬀ by at least√ε(1−ε)i
2
2i+1, so the total (1+ ε)-error
ofAafteristages is at least/summationtexti
k=12k−2/parenleftbigg√ε(1−ε)k
2
2k+1/parenrightbigg1+ε
. Thus
opt1+ε(F∞)≥∞/summationdisplay
k=12k−2/parenleftigg√ε(1−ε)k
2
2k+1/parenrightigg1+ε
=1
2/parenleftbigg√
ε(1−ε)
4/parenrightbigg1+ε
1−2/parenleftig√1−ε
2/parenrightig1+ε= Ω
(ε(1−ε))1+ε
2
1−2/parenleftig√1−ε
2/parenrightig1+ε
.
Sinceεε= Θ(1) and (1 −ε)1+ε= Θ(1) for ε∈/parenleftbig
0,1
2/parenrightbig
, we have opt1+ε(F∞) = Ω/parenleftigg
√ε
1−2/parenleftBig√1−ε
2/parenrightBig1+ε/parenrightigg
.
Since 2ε= Θ(1) for ε∈/parenleftbig
0,1
2/parenrightbig
, we have opt1+ε(F∞) = Ω/parenleftig√ε
2ε−√1−ε1+ε/parenrightig
. Note that (1 −ε)1+ε
2≥
1−ε(1 +ε) forε∈/parenleftbig
0,1
2/parenrightbig
. To check this, note that it is true when ε= 0, and the derivative of
(1−ε)1+ε
2−(1−ε(1+ε)) is
2ε+1+(1 −ε)1+ε
2/parenleftbigg1
2ln(1−ε)−1
2−ε
1−ε/parenrightbigg
>0
forε∈/parenleftbig
0,1
2/parenrightbig
. Thus, opt1+ε(F∞) = Ω/parenleftig√ε
2ε−1+ε(1+ε)/parenrightig
. Also note that 2ε≤1 +εforε∈(0,1).
Equality holds at ε= 0 and ε= 1, and the derivative of 1 + ε−2εis 1−2εln2, which is
positive for ε∈/parenleftbig
0,−lnln2
ln2/parenrightbig
and negative for ε∈/parenleftbig−lnln2
ln2,1/parenrightbig
. Thus 2ε−1 +ε(1 +ε)<3ε, so
opt1+ε(F∞) = Ω(ε−1
2). /squaresolid
Thenext corollary follows from Theorem 2.22, again usingth e fact that optp(F∞)≤optp(Fr)≤
optp(Fq) whenever 1 ≤q≤r.
Corollary 2.23. Ifε∈(0,1), thenopt1+ε(Fq) = Ω(ε−1
2)for allq≥1, where the constant does
not depend on q.
Combining Corollaries 2.20 and 2.23, we have the following t heorem.
Theorem 2.24. Ifε∈(0,1), thenopt1+ε(F∞) = Θ(ε−1
2)andopt1+ε(Fq) = Θ(ε−1
2)for allq≥2,
where the constant in the bound does not depend on q.
3 A multi-variable generalization
In this section, we prove several results on optp(Fq,d). First, we prove a simple lower bound for
optp(Fq,d) in terms of optp(Fq).
Proposition 3.1. For any positive integer d, real number p >0, andq∈[1,∞)∪{∞}, we have
optp(Fq,d)≥dp·optp(Fq).
16Proof.If optp(Fq,d) =∞, there is nothing to prove, and if optp(Fq) =∞, it is clear, by restricting
the inputs xito the set {ce1:c∈[0,1]} ⊂[0,1]d(wheree1∈[0,1]dhas a 1 in the ﬁrst component
and a 0 in the rest), that optp(Fq,d) =∞as well.
Now suppose that both optp(Fq,d) and optp(Fq) are ﬁnite. Fix any algorithm Afor learning
Fq,d. Let1be the all-ones d-tuple and let a(xi: (x0,z0),...,(xi−1,zi−1)) denote the output of A
given the input xi1after learning the pairs ( xj1,zj) forj < i, given that there is a function in Fq,d
which passes through the points ( xj1,zj) forj < i. Then let A′be the algorithm for learning Fq
which, given the input xiafter learning the pairs ( xj,wj) forj < i, returns the output
a′(xi: (x0,w0),...,(xi−1,wi−1)) =a(xi: (x0,dw0),...,(xi−1,dwi−1))
d,
given that there is a function in Fqwhich passes through the points ( xj,wj) forj < i.
Fixε >0. Then there exist f∈ Fqand a sequence of inputs x0,x1,...,x Msuch that
M/summationdisplay
i=1|a′(xi: (x0,f(x0)),...,(xi−1,f(xi−1)))−f(xi)|p≥optp(Fq)−ε.
AgainstA, the adversary uses the function
γ(a1,...,a d) =d/summationdisplay
i=1f(ai)
with the inputs x01,x11,...,x M1. First, suppose that q∈[1,∞). Observe that for any 1 ≤k≤d
andd−1 realsxi∈[0,1], where 1 ≤i≤dbuti/ne}ationslash=k,
/integraldisplay1
0/vextendsingle/vextendsingle/vextendsingle/vextendsingledγ
dxk/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
dxk=/integraldisplay1
0|f′(x)|qdx≤1
sincef∈ Fq; hence,γ∈ Fq,d. Next, suppose that q=∞. Observe that/vextendsingle/vextendsingle/vextendsingledγ
dxk/vextendsingle/vextendsingle/vextendsingle=|f′(xk)| ≤1 for all
xk∈[0,1] sincef∈ Fq; hence, in this case we also have γ∈ Fq,d. To ﬁnish the proof, let
ei=|a′(xi: (x0,f(x0)),...,(xi−1,f(xi−1)))−f(xi)|
and
ki=|a(xi: (x0,γ(x01)),...,(xi−1,γ(xi−11)))−γ(xi1)|
for each i. Thus,
ki=|da′(xi: (x0,f(x0)),...,(xi−1,f(xi−1)))−df(xi)|=dei
and
M/summationdisplay
i=1ep
i≥optp(Fq)−ε.
Hence,
M/summationdisplay
i=1kp
i≥dp(optp(Fq)−ε).
Takingε→0 ﬁnishes the proof. /squaresolid
17The next corollary follows from Proposition 3.1 since optp(F1) =∞[9].
Corollary 3.2. For any positive integer dand real number p >0, we have optp(F1,d) =∞.
Now we directly prove some results about optp(F∞,d), depending on whether p < dorp > d.
The main negative result is the following.
Theorem 3.3. Letd >0be an integer and pbe a real number with 0< p < d. Thenoptp(F∞,d) =
∞.
Proof.Fix any algorithm Afor learning F∞,d. Then choose any integer n≥1, and let Sbe the
set of reals 0 < r <1 such that 2 nris an odd integer (so |S|=n). The adversary ﬁrst reveals
f(0,...,0) = 0, then chooses xiranging over all elements of Sdin lexicographic order, receives
input ˆyifromA, and reveals f(xi) =±1
2n, whichever is farther from ˆ yi.
Let{x}=x−⌊x⌋denote the fractional part of x. At the end of the nd+1 trials, the algorithm’s
revealed values of fare consistent with a function f: [0,1]d→Rgiven by
f(x1,...,x d) =±1
nmin
1≤i≤d(min({nxi},{−nxi})),
where the signs ±are chosen such that f(x) agrees with the adversary’s outputs for any x=
(x1,...,x d)∈Sdandfhas constant sign in any region
/parenleftbiggn1
n,n1+1
n/parenrightbigg
×...×/parenleftbiggnd
n,nd+1
n/parenrightbigg
⊂[0,1]d
for integers 0 ≤ni< n. The consistency follows since {nxi}={−nxi}=1
2for allxi∈S.
First, we show f∈ F∞,d. Fix any 1 ≤i≤dandx= (x1,...,x d−1)∈[0,1]d−1, and consider
the function g: [0,1]→Rgiven by g(x) =f(x′), where x′∈[0,1]dis formed by inserting xinto
theithposition of x. Thengis given by
g(x) =±1
nmin(min( {nx},{−nx}),M),
where
M= min
1≤i≤d−1(min({nxi},{−nxi})).
Evidently gis piecewise linear, with ﬁnitely many points where g′is not deﬁned and |g′(x)|= 1
org′(x) = 0 everywhere else by deﬁnition of g; moreover, since the function min( {nx},{−nx})
is continuous, it follows that gis continuous. Hence g∈ F∞. Since this holds for any choice of
1≤i≤dandx∈[0,1]d, it follows that f∈ F∞,d.
Now we ﬁnd a lower bound for the error the adversary can guaran tee. There are ndtrials past
the ﬁrst, each of which has |ˆyi−f(xi)| ≥1
2n; hence the adversary guarantees
/summationdisplay
i>0|ˆyi−f(xi)|p≥nd
(2n)p=1
2p·nd−p.
Because p < d, this grows arbitrarily large as nincreases; hence optp(F∞,d) =∞. /squaresolid
AsF∞⊆ Fqimplies that F∞,d⊆ Fq,dfor anyq≥1, this bound extends to q/ne}ationslash=∞.
18Corollary 3.4. Letd >0be an integer and pbe a real number with 0< p < d. For any q≥1, we
haveoptp(Fq,d) =∞.
In order to establish upper bounds on optp(F∞,d), we prove the following lemma.
Lemma 3.5. Forf∈ F∞,dandx1= (x1,1,...,x d,1),x2= (x1,2,...,x d,2)∈[0,1]d, we have
|f(x1)−f(x2)| ≤d/summationdisplay
i=1|xi,1−xi,2|.
Proof.Fix such f,x1,x2. Deﬁne a sequence of x′
i∈[0,1]d, for 0≤i≤d, such that x′
ihas its ﬁrst
icomponents equal to the ﬁrst icomponents of x2and its last d−icomponents equal to the last
d−icomponents of x1(sox′
0=x1andx′
d=x2). By the triangle inequality,
|f(x1)−f(x2)|=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingled/summationdisplay
i=1/parenleftbig
f(x′
i−1)−f(x′
i)/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤d/summationdisplay
i=1/vextendsingle/vextendsinglef(x′
i−1)−f(x′
i)/vextendsingle/vextendsingle.
Now consider any 1 ≤i≤d. Note that xi−1andx′
ionly diﬀer in their ithcomponents, with one
beingxi,1and the other being xi,2. Then by deﬁnition of F∞,dand using the fact that for g∈ F∞
andx1,x2∈[0,1],|g(x1)−g(x2)| ≤ |x1−x2|, it follows that/vextendsingle/vextendsinglef(x′
i−1)−f(x′
i)/vextendsingle/vextendsingle≤ |xi,1−xi,2|.
Summing over 1 ≤i≤dyields the result. /squaresolid
Lemma 3.5 makes the class F∞,dparticularly nice to work with. Using a nearest neighbor
algorithm, we establish the following upper bound.
Theorem 3.6. Suppose p > d. Thenoptp(F∞,d)≤(2d−1)dp
1−2d
2p.
Proof.Consider the algorithm Awhich guesses 0 on the ﬁrst input and, on trial i(after receiving
inputsx0,...,xi−1), picks the least index 0 ≤j < iwhich minimizes the L1distance between xj
andxiand guesses ˆ yi=f(xj). We will show Lp(A,F∞,d)≤(2d−1)dp
1−2d
2p.
Fixf∈ F∞,dand a sequence x0,...,xmofxi∈[0,1]d. Assume all the xiare distinct. Then
for each 1 ≤i≤m, there exists a least integer kisuch that, if [0 ,1]dis divided into 2kidregions
given by
{(x1,...,x d)∈[0,1]d:ni≤2kixi≤ni+1}
over alld-tuples (n1,...,n d) of integers 0 ≤ni<2ki, thenxiis not in the same region as any of
x0,...,xi−1. Note that because x0andxiare both in [0 ,1]dfor any 1 ≤i≤m, allkiare at least
1. For each integer k≥1, letckbe the number of integers 1 ≤i≤msuch that ki=k. By the
Pigeonhole Principle, for any ﬁxed integer k≥0, there exist at most 2kd−1 indices 1 ≤i≤m
such that ki≤k; otherwise, at least 2kd+1 of the xi(including x0) would be the ﬁrst within their
containing length-2−khypercube region. Thus
K/summationdisplay
k=1ck≤2Kd−1 (4)
for any integer K≥1. Moreover, for any 1 ≤i≤m,xilies in the same length-2−(ki−1)hypercube
as one of x0,...,xi−1, andthis hypercubehas L1distanced
2ki−1between two of its oppositevertices,
so by Lemma 3.5,
|ˆyi−f(xi)| ≤d
2ki−1. (5)
19Combining these,
m/summationdisplay
i=1|ˆyi−f(xi)|p≤/summationdisplay
k≥1ck/parenleftbiggd
2k−1/parenrightbiggp
=/summationdisplay
K≥1/bracketleftigg/parenleftiggK/summationdisplay
k=1ck/parenrightigg/parenleftbigg/parenleftbiggd
2K−1/parenrightbiggp
−/parenleftbiggd
2K/parenrightbiggp/parenrightbigg/bracketrightigg
≤dp/summationdisplay
K≥1(2Kd−1)(2−p(K−1)−2−pK) =dp(2p−1)/summationdisplay
K≥12−pK(2Kd−1)
=dp(2p−1)/parenleftbigg2d−p
1−2d−p−2−p
1−2−p/parenrightbigg
=(2d−1)dp
1−2d
2p.
This holds for all f∈ F∞,dand sequences of xi; hence Lp(A,F∞,d)≤(2d−1)dp
1−2d
2p. /squaresolid
The next corollary follows from Proposition 3.1 and Theorem 3.6 since optp(F∞) = 1 for all
p≥2.
Corollary 3.7. For any ﬁxed positive integer dand real number p≥d+1, we have optp(F∞,d) =
Θ(dp), where the constant in the upper bound depends only on d.
We can also use Theorems 3.3 and 3.6 to obtain sharp bounds on t he worst-case errors for
learning F∞,dwhen the number of trials is bounded.
Corollary 3.8. Letd >0be an integer and pbe areal number with 0< p < d. Thenoptp(F∞,d,m) =
Θ(m1−p
d), where the constants in the bounds depend on pandd.
Proof.By Theorem 3.3, we have optp(F∞,d,m)≥1
2pnd−pforn=⌊m1/d⌋, so we obtain the lower
bound optp(F∞,d,m)≥1
2pmd−p
d(1−o(1)). For the upper bound, we use the algorithm and notation
of Theorem 3.6 with K=/ceilingleftig
log2(m+1)
d/ceilingrightig
to obtain
m/summationdisplay
i=1|ˆyi−f(xi)|p≤m/summationdisplay
i=1/parenleftbiggd
2ki−1/parenrightbiggp
≤K/summationdisplay
k=1(2kd−2(k−1)d)/parenleftbiggd
2k−1/parenrightbiggp
=dp(2d−1)K/summationdisplay
k=12(k−1)(d−p)=dp(2d−1)2K(d−p)−1
2d−p−1
<dp(2d−1)2d−p
2d−p−1md−p
d(1+o(1)),
wheretheﬁrstinequality follows fromInequality 5andthes econd inequality follows fromInequality
4 since/parenleftbigd
2k−1/parenrightbigpis decreasing in k. Thus
optp(F∞,d,m)≤dp(2d−1)2d−p
2d−p−1md−p
d(1+o(1)).
/squaresolid
204 Discussion and open problems
With the results in this paper, the value of optp(Fq) is now bounded up to a constant factor for all
p,q≥1 except when q∈(1,2) andp∈(1,2)∪(2,2+1
q−1). In particular, by combining the results
in this paper with the results in [9], we now know that optp(Fq) = 1 for all ( p,q) that lie in the
following regions.
•p,q≥2
•q∈(1,2) andp≥2+1
q−1
Inadditiontoinvestigating theregions inwhichoptp(Fq)is notboundedupto aconstant factor,
itremainstonarrowtheconstantgapbetweentheupperandlo werboundsforopt1+ε(Fq) = Θ(ε−1
2)
whenε∈(0,1) andq∈[2,∞)∪ {∞}. Another similar problem is to narrow the constant gap
between the upper and lower bounds for opt2(Fq) = Θ(ε−1) whenq∈(1,2).
The results in this paper also help characterize the values o f (p,q) for which optp(Fq) is ﬁnite.
Before this paper, it was only known that opt2(Fq) is ﬁnite for p >1 andq≥2, and optp(Fq) =∞
whenp= 1 orq= 1. With our new results, we now know that optp(Fq) is also ﬁnite when p≥2
andq >1. We make the following conjecture about this problem.
Conjecture 4.1. For allp >1andq >1,optp(Fq)is ﬁnite.
Besides the new results about smooth functions of a single va riable, we also introduced a
generalization of the model to multi-variable functions an d found some bounds for this multi-
variable online learning scenario. We showed that optp(F∞,d) is inﬁnite when 0 < p < d and ﬁnite
whenp > d, but it remains to determine whether optd(F∞,d) is ﬁnite for d >1. For ﬁnite q≥1 and
0< p < d, we also know that optp(Fq,d) is inﬁnite, but it remains to determine whether optp(Fq,d)
is ﬁnite for p≥dandq∈(1,∞). In addition, we proved for any ﬁxed positive integer dthat
optp(F∞,d) = Θ(dp) forp≥d+ 1, where the constant in the upper bound depends on d. The
multiplicative gap between the upper and lower bounds is 2d+1−2. We conjecture that the lower
bound is sharp for psuﬃciently large with respect to dandq.
Conjecture 4.2. For allq∈[1,∞)∪{∞}, for all positive integers d, and for all real numbers p
suﬃciently large with respect to qandd, we have optp(Fq,d) =dp.
The papers [9] and [12] investigated opt1(Fq,m) forq≥2, where mis the number of trials. It
would be natural to study optp(Fq,m) forp= 1+εwith 0< ε <1 andq≥1, since opt1+ε(Fq) can
grow arbitrarily large as ε→0. We bounded optp(F∞,d,m) up to a constant factor for any ﬁxed
positive integer dand ﬁxed real number pwith 0< p < d, but the constants in the bounds depend
onpandd. It remains to narrow the gap between the upper bound ofdp(2d−1)2d−p
2d−p−1md−p
d(1+o(1))
and the lower bound of1
2pmd−p
d(1−o(1)). It would also be interesting to investigate optp(Fq,d,m)
for ﬁnite values of q.
Another possible direction would be to investigate familie s of smooth functions with additional
restrictions. For example, let Eq⊆ Fqbe the family of exponential functions f(x) =eax+bwith
f∈ Fq.
Proposition 4.3. For allp >0andq≥1, we have optp(Eq) = 1.
21Proof.The upper bound optp(Eq)≤1 follows by Lemma 2.5, since the learner knows the function
after two rounds with diﬀerent inputs and the ﬁrst round does n ot count for the total error. For
the lower bound, consider an adversary that chooses some ε∈(0,1), deﬁnes ̺= 1−1−ε√1−ε, and
revealsf(0) =−1−ε
ln(1−ε). On the second turn, they either reveal f(1) =−1
ln(1−ε)orf(1) =−1−̺
ln(1−̺),
whichever maximizes the error for the learner’s guess.
Iff(1) =−1
ln(1−ε), thenf(x) =eax+bwitha=−ln(1−ε) andb= ln(1−ε)−ln(−ln(1−ε)).
Note that f′(x) =aeax+b∈[1−ε,1] for all x∈[0,1], sof∈ Eqforallq≥1. Iff(1) =−1−̺
ln(1−̺), then
f(x) =eax+bwitha= ln(1−̺) andb=−ln(−ln(1−̺)). Note that f′(x) =aeax+b∈[−1,−1+̺]
for allx∈[0,1], sof∈ Eqfor allq≥1. Moreover, note that
lim
ε→0/parenleftbigg
−1
ln(1−ε)+1−̺
ln(1−̺)/parenrightbigg
= lim
ε→0/parenleftbigg−1+(1−ε)1−ε√1−ε
ln(1−ε)/parenrightbigg
= 2,
by L’Hˆ opital’s rule. Thus optp(Eq)≥1. /squaresolid
LetPq,m⊆ Fqbe the family of polynomial functions f∈ Fqof degree at most m. It is easy to
see that optp(Pq,1) = 1 for all p >0 andq≥1, but it would be interesting to investigate optp(Pq,m)
form >1. Note that we have optp(Pq,m)≤optp(Fq,m) for allp >0,q≥1, andm≥1, since the
learner will know f∈ Pq,mwith certainty after being tested on m+1 diﬀerent inputs. Let Pq⊆ Fq
be the family of all polynomial functions f∈ Fq. We make the following conjecture about this
family.
Conjecture 4.4. For allp >0andq≥1, we have optp(Pq) = optp(Fq).
Some other possible subsets of Fqthat could be investigated are piecewise functions with at
mostkpieces where the pieces are polynomials of degree at most m, sums of exponential functions,
and sums of trigonometric functions.
Finally, wereturnto theproblemfromtheintroduction ofpr edictingthenextday’s temperature
range at a given location. In particular, consider the singl e-variable problem where we predict the
next day’s temperature range based only on the time of year. A n issue with using the model from
[9] for this temperature prediction problem is that the same input for time of year could have
diﬀerent outputs for the temperature range in diﬀerent years. A more realistic way to model this
problem would be to choose the output from a probability dist ribution which depends on the input.
In order for the learner to be able to guarantee a ﬁnite bound o n the worst-case error, the number
of trials would be bounded and restrictions would be placed o n the probability distribution. For
example, the density function for the probability distribu tion could be required to have smoothness
properties like the functions from [9], and the support of th e density function could be required to
be a subset of [0 ,r] for some r >0. Investigating such a model would be an interesting direct ion
for future research. Note that this model reduces to the mode l from [9] when the support consists
of a single point.
5 Acknowledgments
Most of this research was performed in PRIMES 2022. We thank t he organizers for this research
opportunity. Our paper subsumes [5], which proved Theorem 1 .2. We also thank the anonymous
reviewers for helpful comments which improved the clarity a nd presentation of the results in this
paper.
22References
[1] D. Angluin, Queries and concept learning. Machine Learning 2(1988) 319–342.
[2] A. Barron, Approximation and estimation bounds for arti ﬁcial neural networks. Workshop on
Computational Learning Theory (1991)
[3] N. Cesa-Bianchi, P.M. Long, and M.K. Warmuth, Worst-cas e quadratic loss bounds for pre-
diction using linear functions and gradient descent. IEEE T ransactions on Neural Networks 7
(1996) 604–619.
[4] V. Faber and J. Mycielski, Applications of learning theo rems. Fundamenta Informaticae 15
(1991) 145–167.
[5] J. Geneson, Sharper bounds for online learning of smooth functions of a single variable. CoRR
abs/2105.14648 (2021)
[6] W. Hardle, Smoothing techniques. Springer Verlag (1991 )
[7] D. Haussler, Generalizing the PAC model: sample size bou nds from metric dimension-based
uniform convergence results. Proceedings of the 30th Annual Symposium on the Foundations of
Computer Science (1989)
[8] S. Kaczmarz, Angenaherte Auﬂ¨ osung von systemen linear er gleichungen. Bull. Acad. Polon.
Sci. Lett. A 35(1937) 355–357.
[9] D. Kimber and P. M. Long, On-line learning of smooth funct ions of a single variable. Theoretical
Computer Science 148(1995) 141–156.
[10] N. Littlestone, Learning quickly when irrelevant attr ibutes abound: A new linear-threshold
algorithm. Machine Learning 2(1988) 285–318.
[11] N. Littlestone and M.K. Warmuth, The weighted majority algorithm. Proceedings of the 30th
Annual Symposium on the Foundations of Computer Science (1989)
[12] P. M. Long, Improved bounds about on-line learning of sm ooth functions of a single variable.
Theoretical Computer Science 241(2000) 25–35.
[13] J. Mycielski, A learning algorithm for linear operator s.Proceedings of the American Mathe-
matical Society 103(1988) 547–550.
[14] B. Widrow and M.E. Hoﬀ, Adaptive switching circuits. 1960 IRE WESCON Conv. Record
(1960) 96–104.
23