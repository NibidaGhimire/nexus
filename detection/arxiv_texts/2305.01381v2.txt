Sample EfÔ¨Åcient Model-free Reinforcement Learning from
LTL SpeciÔ¨Åcations with Optimality Guarantees
Daqian Shao and Marta Kwiatkowska
Department of Computer Science, University of Oxford, UK
fdaqian.shao, marta.kwiatkowska g@cs.ox.ac.uk
Abstract
Linear Temporal Logic (LTL) is widely used to
specify high-level objectives for system policies,
and it is highly desirable for autonomous systems to
learn the optimal policy with respect to such spec-
iÔ¨Åcations. However, learning the optimal policy
from LTL speciÔ¨Åcations is not trivial. We present a
model-free Reinforcement Learning (RL) approach
that efÔ¨Åciently learns an optimal policy for an un-
known stochastic system, modelled using Markov
Decision Processes (MDPs). We propose a novel
and more general product MDP, reward structure
and discounting mechanism that, when applied in
conjunction with off-the-shelf model-free RL al-
gorithms, efÔ¨Åciently learn the optimal policy that
maximizes the probability of satisfying a given
LTL speciÔ¨Åcation with optimality guarantees. We
also provide improved theoretical results on choos-
ing the key parameters in RL to ensure optimality.
To directly evaluate the learned policy, we adopt
probabilistic model checker PRISM to compute the
probability of the policy satisfying such speciÔ¨Åca-
tions. Several experiments on various tabular MDP
environments across different LTL tasks demon-
strate the improved sample efÔ¨Åciency and optimal
policy convergence.
1 Introduction
Linear Temporal Logic (LTL) is a temporal logic language
that can encode formulae regarding properties of an inÔ¨Å-
nite sequence of logic propositions. LTL is widely used for
the formal speciÔ¨Åcation of high-level objectives for robotics
and multi-agent systems, and it is desirable for a system
or an agent in the system to learn policies with respect to
these high-level speciÔ¨Åcations. Such systems are modelled
as Markov Decision Processes (MDPs), where classic policy
synthesis techniques can be adopted if the states and transi-
tions of the MDP are known. However, when the transitions
are not known a priori , the optimal policy needs to be learned
through interactions with the MDP.
Model-free Reinforcement Learning [Sutton and Barto,
2018 ], a powerful method to train an agent to choose actionsin order to maximize rewards over time in an unknown envi-
ronment, is a perfect candidate for LTL speciÔ¨Åcation policy
learning. However, it is not straightforward to utilize reward-
based RL to learn the optimal policy that maximizes the prob-
ability of satisfying LTL speciÔ¨Åcations [Alur et al. , 2021 ]due
to the difÔ¨Åculty of deciding when, where, and how much re-
ward to give to the agent. To this end, most works adopt a
method to Ô¨Årst transform the LTL speciÔ¨Åcation into an au-
tomaton, then build a product MDP using the original envi-
ronment MDP and the automaton, on which model-free RL
algorithms are applied. However, one crucial obstacle still
remains, and that is how to properly deÔ¨Åne the reward that
leads an agent to the optimal satisfaction of LTL speciÔ¨Åcation.
Several algorithms have been proposed [Hahn et al. , 2019;
Bozkurt et al. , 2019; Hasanbeig et al. , 2019 ]for learning LTL
speciÔ¨Åcations, where, in order to ensure optimality, the key
parameters are chosen depending on assumptions or knowl-
edge of the environment MDP. These works normally only
prove the existence of the optimality guarantee parameters or
provide unnecessarily harsh bounds for them, which might
lead to inefÔ¨Åcient learning. In addition, it is unclear how to
explicitly choose these parameters even with certain knowl-
edge of the environment MDP. Furthermore, the assumed
parameters are evaluated in experiments indirectly, either
through inspection of the value function or comparison of the
expected reward gained, making it difÔ¨Åcult to tune the opti-
mality parameters for LTL learning in general MDPs.
In this work, we propose a novel and more general prod-
uct MDP, reward structure and discounting mechanism that,
by leveraging model-free reinforcement learning algorithms,
efÔ¨Åciently learns the optimal policy that maximizes the prob-
ability of satisfying the LTL speciÔ¨Åcation with guarantees.
We demonstrate improved theoretical results on the optimal-
ity of our product MDP and the reward structure, with more
stringent analysis that yields better bounds on the optimality
parameters. Moreover, this analysis sheds light on how to
explicitly choose the optimality parameters based on the en-
vironment MDP. We also adopt counterfactual imagining that
exploits the known high-level LTL speciÔ¨Åcation to further im-
prove the performance of our algorithm. Last but not least,
we propose to use the PRISM model checker [Kwiatkowska
et al. , 2011 ]to directly evaluate the satisfaction probability
of the learned policies, providing a platform to directly com-
pare algorithms and tune key parameters. We conduct exper-arXiv:2305.01381v2  [cs.LG]  3 May 2023iments on several common MDP environments with various
challenging LTL tasks, and demonstrate the improved sample
efÔ¨Åciency and convergence of our methods.
Our contributions include: (i) a novel product MDP design
that incorporates an accepting states counter with a gener-
alized reward structure; (ii) a novel reinforcement learning
algorithm that converges to the optimal policy for satisfy-
ing LTL speciÔ¨Åcations, with theoretical optimality guaran-
tees and theoretical analysis results on choosing the key pa-
rameters; (iii) the use of counterfactual imagining, a method
to exploit the known structure of the LTL speciÔ¨Åcation by
creating imagination experiences through counterfactual rea-
soning; and (iv) direct evaluation of the proposed algorithms
through a novel integration of probabilistic model checkers
within the evaluation pipeline, with strong empirical results
demonstrating better sample efÔ¨Åciency and training conver-
gence.
Related Work
Most works on LTL learning with reward-based RL utilize
a product MDP: a product of the environment MDP and an
automaton translated from the LTL speciÔ¨Åcation. Sadigh et
al.[2014 ]Ô¨Årst used deterministic Rabin automata to create
this product with a discounted reward design to learn LTL,
while later works adopted a new automaton design, limit-
deterministic B ¬®uchi automata (LDBA) [Sickert et al. , 2016 ].
Hahn et al. [2019 ]adopted a product MDP with LDBA and
augmented it with sink states to reduce the LTL satisfaction
problem into a limit-average reward problem with optimal-
ity guarantees. Hahn et al. [2020 ]later modiÔ¨Åed this ap-
proach by including two discount factors with similar opti-
mality guarantee results. Bozkurt et al. [2019 ]proposed a
discounted reward learning algorithm on the product MDP
with optimality guarantees, where the discount factor is cho-
sen based on certain assumptions about the unknown environ-
ment MDP. To the best of our knowledge, these approaches
are the only available to provide optimality guarantees for the
full inÔ¨Ånite-horizon LTL learning. However, many methods
have nevertheless demonstrated empirical results for learning
LTL. Hasanbeig et al. [2020; 2019 ]proposed an accepting
frontier function as the reward for the product MDP, while
Caiet al. [2021 ]extended this reward frontier to continuous
control tasks.
Due to the difÔ¨Åculty of learning full LTL, many approaches
focus on learning restricted Ô¨Ånite LTL variants. Giacomo et
al.[2013; 2019 ]proposed the LTLf variant and a correspond-
ing reinforcement learning algorithm; Littman et al. [2017 ]
formulated a learning algorithm for the GLTL variant; Ak-
saray et al. [2016 ]proposed to learn Signal Temporal Logic
and Li et al. [2016 ]a truncated LTL variant for robotics ap-
plications.
Another related line of work leverages automata to learn
non-Markovian rewards. Toro Icarte et al. [2022; 2018 ]de-
Ô¨Åned a reward machine automaton to represent high-level
non-Markovian rewards, while Camacho et al. [2019 ]intro-
duced a method to learn Ô¨Ånite LTL speciÔ¨Åcations by trans-
forming them into reward machines. However, the expres-
siveness of reward machines is strictly weaker than that of
LTL. Lastly, there are works that exploit other high-level logicspeciÔ¨Åcations to facilitate learning [Andreas et al. , 2016;
Jiang et al. , 2021; Jothimurugan et al. , 2020; Jothimurugan
et al. , 2021 ], but they are less relevant to reinforcement learn-
ing from LTL.
2 Preliminaries
Before formulating our problem, we provide preliminary
background on Markov decision processes, linear temporal
logic, and reinforcement learning.
2.1 Markov Decision Processes
DeÔ¨Ånition 1 (Markov decision process [Littman, 2001 ]).
A Markov decision process (MDP) M is a tuple
(S;s0;A;T;AP;L;r; ), whereSis a Ô¨Ånite set of states,
s02Sis the initial state, Ais a Ô¨Ånite set of actions,
T:SAS![0;1]is the probabilistic transition func-
tion,AP is the set of atomic propositions, L:S!2APis
the proposition labeling function, r:SAS!Ris a
reward function and :S!(0;1]is a discount function.
LetA(s)denote the set of available actions at state s, then,
for alls2S, it holds thatP
s02ST(s;a;s0) = 1 ifa2A(s)
and 0 otherwise.
An inÔ¨Ånite path is a sequence of states =s0;s1;s2:::,
where there exist ai+12A(si)such thatT(si;ai+1;si+1)>
0for alli0, and a Ô¨Ånite path is a Ô¨Ånite such sequence. We
denote the set of inÔ¨Ånite and Ô¨Ånite paths of the MDP Mas
PathsMandFPathsM, respectively. We use [i]to denote
si, and[:i]and[i:]to denote the preÔ¨Åx and sufÔ¨Åx of
the path, respectively. Furthermore, we assume self-loops: if
A(s) =?for some state s, we letT(s;a;s ) = 1 for some
a2AandA(s) =asuch that all Ô¨Ånite paths can be extended
to an inÔ¨Ånite one.
A Ô¨Ånite-memory policy forMis a function :
FPathsM!D(A)such thatsupp(())A([ 1]),
whereD(A)denotes a distribution over A,supp(d)denotes
the support of the distribution and [ 1]is the last state of a
Ô¨Ånite path. A policyis memoryless if it only depends on
the current state, i.e.,[ 1] =0[ 1]implies() =(0),
and a policy is deterministic if ()is a point distribution for
all2FPathsM. For a deterministic memoryless policy,
we let(s)represent()where[ 1] =s.
LetPathsM
PathsMdenote the subset of inÔ¨Ånite
paths that follow policy and we deÔ¨Åne the probability space
(PathsM
;FPathsM;P)overPathsM
in the standard way.
Then, for any function f:PathsM
!R, letE[f]be the
expectation of fover the inÔ¨Ånite paths of Mfollowing.
A Markov chain (MC) induced by Mand deterministic
memoryless policy is a tupleM= (S;s0;T;AP;L),
whereT(s;s0) =T(s;(s);s0). A sink (bottom) strongly
connected component (BSCC) of a MC is a set of states
CSsuch that, for all pairs s1;s22C, there exists a path
froms1tos2following the transition function T(strongly
connected), and there exists no state s02SnCsuch that
T(s;s0)>0for alls2C(sink).2.2 Linear Temporal Logic
Linear Temporal Logic (LTL) provides a high-level descrip-
tion for speciÔ¨Åcations of a system. LTL is very expressive and
can describe speciÔ¨Åcations with inÔ¨Ånite horizon.
DeÔ¨Ånition 2 ([Baier and Katoen, 2008 ]).An LTL formula
over atomic propositions APis deÔ¨Åned by the grammar:
'::=truejpj'1^'2j:'jX'j'1U'2; p2AP;
where Xrepresents next and Urepresents until. Other
Boolean and temporal operators are derived as follows: or:
'1_'2=:(:'1^:'2); implies:'1!'2=:'1_'2;
eventually: F'=trueU'; and always: G'=:(F:').
The satisfaction of an LTL formula 'by an inÔ¨Ånite path
2PathsMis denoted by j=', and is deÔ¨Åned by induc-
tion on the structure of ':
satisÔ¨Åes'ifp2L([0]) forp2AP ;
[1 :]j=' forX';
9i;[i]j='2and8j <i; [j]j='1for'1U'2;
with the satisfaction of Boolean operators deÔ¨Åned by their
default meaning.
2.3 Reinforcement Learning
Reinforcement learning [Sutton and Barto, 2018 ]teaches an
agent in an unknown environment to select an action from
its action space, in order to maximize rewards over time. In
most cases the environment is modelled as an MDP M=
(S;s0;A;T;AP;L;r; ). Given a deterministic memoryless
policy, at each time step t, let the agent‚Äôs current state be st,
then the action a=(st)is chosen and the next state st+1s
T(st;a;)together with the immediate reward r(st;a;st+1)
is received from the environment. Then, starting at s2Sand
time stept, the expected discounted reward following is
G
t(s) =E[1X
i=t(i 1Y
j=t(sj))r(si;ai;si+1)jst=s];(1)
whereQt 1
j=t:= 1. The agent‚Äôs goal is to learn the opti-
mal policy that maximizes the expected discounted reward.
Note that we deÔ¨Åned a discount function instead of a constant
discount factor because it is essential for our proposed LTL
learning algorithm to discount the reward depending on the
current MDP state.
Q-learning [Watkins and Dayan, 1992 ]is a widely used ap-
proach for model-free RL. It utilizes the idea of the Q function
Q(s;a), which is the expected discounted reward of taking
actionaat statesand following policy after that. The Q
function for all optimal policies satisÔ¨Åes the Bellman opti-
mality equations:
Q(s;a) =X
s02ST(s;a;s0) 
r(s;a;s0) (2)
+(s) max
a02AQ(s0;a0)
8a2A;s2S:
At each iteration of the Q-learning algorithm, the agent‚Äôs
experiences, i.e, the next state s0and immediate reward
r(s;a;s0), are used to update the Q function:
Q(s;a) r(s;a;s0) +(s) max
a02AQ(s0;a0); (3)whereis the learning rate and x yrepresentsx x+
(y x). In addition, the optimal policy can be recovered
from the optimal Q function Q(s;a)by selecting the action
awith the highest state-action pair value in each state s. Q-
learning converges to the optimal Q function in the limit given
that each state-action pair is visited inÔ¨Ånitely often [Watkins
and Dayan, 1992 ], and thus learns the optimal policy.
3 Our Method
Our goal is to formulate a model-free reinforcement learn-
ing approach to efÔ¨Åciently learn the optimal policy that max-
imizes the probability of satisfying an LTL speciÔ¨Åcation with
guarantees. We now give an overview of our method. We
Ô¨Årst transform the LTL objective 'into a limit-deterministic
B¬®uchi automaton. Then, we introduce a novel product MDP
and deÔ¨Åne a generalized reward structure on it. With this re-
ward structure, we propose a Q-learning algorithm that adopts
a collapsed Q function to learn the optimal policy with opti-
mality guarantees. Lastly, we enhance our algorithm with
counterfactual imagining that exploits the automaton struc-
ture to improve performance while maintaining optimality.
3.1 Problem Formulation
Given an MDPM= (S;s0;A;T;AP;L;r; )with un-
known states and transitions and an LTL objective ', for any
policyof the MDPM, letP(sj=')denote the prob-
ability of paths from state s following satisfying the LTL
formula':
P(sj=') =Pf2PathsM
j[0] =s;j='g:(4)
Then, we would like to design a model-free RL algorithm
that learns a deterministic memoryless optimal policy 'that
maximizes the probability of Msatisfying':
P'(sj=') = max
P(sj=')8s2S: (5)
3.2 Limit-deterministic B ¬®uchi automata
We Ô¨Årst transform the LTL speciÔ¨Åcations into automata. The
common choices of automata include deterministic Rabin au-
tomata and non-deterministic B ¬®uchi automata. In this work,
we adopt a B ¬®uchi automata variant called limit-deterministic
B¬®uchi automata (LDBA) [Sickert et al. , 2016 ].
DeÔ¨Ånition 3. A non-deterministic B ¬®uchi automaton is an au-
tomatonA= (AP;Q;q0;;F), whereAP is the set of
atomic propositions, Qis a Ô¨Ånite set of states, q02 Q is
the initial state and F  Q is the set of accepting states.
Let = 2AP[fgbe a Ô¨Ånite alphabet, then the transition
function is given by  :Q!2Q.
DeÔ¨Ånition 4 (LDBA) .A B ¬®uchi automaton is limit-
deterministic ifQcan be partitioned into a deterministic set
and a non-deterministic set, that is, Q=QN[QD, where
QN\QD=?, such that
1.FQDand q02QN;
2.j(q;)j1for all q2QNand6=2;
3.(q;)QDandj(q;)j1for all q2QDand
2;An LDBA is a B ¬®uchi automaton where the non-
determinism is limited in the initial component QNof the
automaton. An LDBA starts in a non-deterministic initial
component and then transitions into a deterministic accept-
ing componentQDthrough-transitions after reaching an
accepting state, where all transitions after this point are deter-
ministic. We follow the formulation of Bozkurt et al. [2019 ]
to extend the alphabets with an -transition that handles all the
non-determinism, meaning only can transition the automa-
ton state to more then 1 states: j(q;)j>1. This allows
the MDP to synchronise with the automaton, which we will
discuss in detail in Section 3.3.
An inÔ¨Ånite wordW2 !, where !is the set of all in-
Ô¨Ånite words over the alphabet , is accepted by a B ¬®uchi au-
tomatonAif there exists an inÔ¨Ånite automaton run 2Q!
from q0, where[t+ 1]2([t];W[t]);8t0, such that
inf()\F6 =?, where inf ()is the set of automaton states
that are visited inÔ¨Ånitely often in the run .
LDBAs are as expressive as the LTL language, and the sat-
isfaction of any given LTL speciÔ¨Åcation 'can be evaluated
on the LDBA derived from '. We use Rabinizer 4 [KÀáret¬¥ƒ±nsk¬¥y
et al. , 2018 ]to transform LTL formulae into LDBAs. In Fig-
ure 1a we give an example of the LDBA derived from the LTL
formula ‚Äú FGa &G!c‚Äù, where state 1 is the accepting state.
LDBAs are different from reward machines [Toro Icarte et
al., 2022 ]because they can express properties satisÔ¨Åable by
inÔ¨Ånite paths, which is strictly more expressive than reward
machines, and they have different accepting conditions.
3.3 Product MDP
In this section, we propose a novel product MDP of the en-
vironment MDP, an LDBA and an integer counter, where the
transitions for each component are synchronised. Contrary to
the standard product MDP used in the literature [Bozkurt et
al., 2019; Hahn et al. , 2019; Hasanbeig et al. , 2020 ], this
novel product MDP incorporates a counter that counts the
number of accepting states visited by paths starting at the ini-
tial state.
DeÔ¨Ånition 5 (Product MDP) .Given an MDP M =
(S;s0;A;T;AP;L;r; ), an LDBAA= (AP;Q;q0;;F)
andK2N, we construct the product MDP as follows:
M=MA [0::K] = (S;s
0;A;T;F;r;);
where the product states S=SQ [0::K], the initial state
s
0= (s0;q0;0), the product actions A=A[fqjq2Qg ,
the accepting setF=SF [0::K], and the product
transitionsT:SAS![0;1], which are deÔ¨Åned
as:
T((s;q;n);a;(^s;^q;n)) (6)
=8
<
:T(s;a;^s)ifa2Aand^q2(q;L(s))nF;
1 ifa=^q;^q2(q;)and^s=s;
0 otherwise:
T((s;q;n);a;(^s;^q;min(n+ 1;K))) (7)
=T(s;a;^s)ifa2Aand^q2(q;L(s))\F;
0 otherwise:where all other transitions are equal to 0. The product reward
r:SAS!Rand the product discount function
:S!(0;1]that are suitable for LTL learning are
deÔ¨Åned later in DeÔ¨Ånition 6.
Furthermore, an inÔ¨Ånite path ofMsatisÔ¨Åes the B ¬®uchi
condition'Fifinf()\F6=?. With a slight abuse of
notation we denote this condition in LTL language as j=
GF'F, meaning for all M2N, there always exists s2
Fthat will be visited in [M:].
When an MDP action a2Ais taken in the product MDP
M, the alphabet used to transition the LDBA is deduced
by applying the label function to the current environment
MDP state: L(s)22AP. In this case, the LDBA tran-
sition(q;L(s))is deterministic. Otherwise, if an -action
^q2fqjq2Qg is taken, LDBA is transitioned with an -
transition, and the non-determinism of (q;)is resolved by
transitioning the automaton state to ^q. TheKcounter value
is equal to 0 in the initial state, and each time an accepting
state is reached, the counter value increases by one until it is
capped atK.
Example 1. We motivate our product MDP structure of Def-
inition 5 through an example. In Figure 1b, we have a grid
environment where the agent can decide to go up, down, left
or right. The task is to visit states labeled ‚Äúa‚Äù inÔ¨Ånitely often
without visiting ‚Äúc‚Äù as described by the LDBA in Figure 1a.
The MDP starts at (1,0), with walls denoted by solid gray
squares. The states in the Ô¨Årst row only allow action right as
denoted by the right pointing triangle, which leads to a sink
at (0,9). There is also a probabilistic gate at (2,0) that transi-
tions the agent randomly to go down or right, and if the agent
reaches (2,1), the accepting sink at (2,9) is reachable. There-
fore, the optimal policy is to go down from the start and stay
in (2,9) if the probabilistic gate at (2,0) transitions the agent
to the right. The probability of satisfying this task is the prob-
ability of the gate sending you to the right. Intuitively, this
environment has some initial accepting states that are easy
to explore, but lead to non-accepting sinks, whereas the true
optimal path requires more exploration. If we set K= 10
in the product MDP for this task, we can assign very small
rewards for the initially visited accepting states and gradu-
ally increase the reward as more accepting states are visited
to encourage exploration and guide the agent to the optimal
policy.
Next, we provide a theorem, which states that the product
MDP with B ¬®uchi condition 'Fis equivalent, in terms of the
optimal policy, to the original MDP with LTL speciÔ¨Åcation '.
The proof of this theorem is provided in Appendix A.1.
Theorem 1 (SatisÔ¨Åability Equivalence) .For any product
MDPMthat is induced from LTL formula ', we have that
sup
P(s0j=') = sup
P(s
0j=GF'F): (8)
Furthermore, a deterministic memoryless policy that maxi-
mizes the probability of satisfying the B ¬®uchi condition 'Fon
the product MDPM, starting from the initial state, induces
a deterministic Ô¨Ånite-memory optimal policy that maximizes
the probability of satisfying 'on the original MDP Mfrom
the initial state.(a) LDBA for ‚Äú FGa &G!c‚Äù.
 (b) A motivating MDP example for the Kcounter, where the purple triangle at (1,0) is
the starting point and the bidirectional arrow at (2,0) is a probability gate.
Figure 1: An example LDBA (left) and probabilistic gate MDP (right) motivating the Kcounter (see Example 1).
Reward Structure for LTL learning
We Ô¨Årst deÔ¨Åne a generalized reward structure in Mfor LTL
learning, and then prove the equivalence between acquiring
the highest expected discounted reward and achieving the
highest probability of satisfying 'Funder this reward struc-
ture.
DeÔ¨Ånition 6 (Reward Structure) .Given a product MDP M
and a policy , the product reward function r:SA
S!Ris suitable for LTL learning if
r((s;q;n);a;(s0;q0;n0)) =Rnifq02F;
0 otherwise,(9)
whereRn2(0;U]are constants for n2[0::K]andU2
(0;1]is an upper bound on the rewards. The rewards are
non-zero only for accepting automaton states, and depend on
the value of the Kcounter.
Then, given a discount factor 2(0;1], we deÔ¨Åne the
product discount function :S!(0;1]as
(s
j) =1 r
j ifr
j>0;
 otherwise,
and the expected discounted reward following policy start-
ing atsand time step tis
G
t(s) =E[1P
i=t(i 1Q
j=t(s
j))r(s
i;a
i;s
i+1)js
t=s]:
(10)
The highest Kvalue reached in a path (i.e., the number
of accepting states visited in the path) acts as a measure of
how promising that path is for satisfying 'F. By exploit-
ing it, we can assign varying rewards to accepting states to
guide the agent, as discussed in the motivating example in
Section 3.3. Next, we provide a lemma stating the properties
of the product MDP regarding the satisfaction of the B ¬®uchi
condition'F.
Lemma 1. Given a product MDP Mwith its correspond-
ing LTL formula 'and a policy , we writeM
for the in-
duced Markov chain from . LetB
Fdenote the set of states
that belong to accepting BSCCs of M
, andB
?denote the
set of states that belong to rejecting BSCCs:
B
F:=fsjs2B2BSCC (M
);B\F6=?g;(11)
B
?:=fsjs2B2BSCC (M
);B\F=?g;(12)whereBSCC (M
)is the set of all BSCCs of M
. We fur-
ther deÔ¨Åne more general accepting and rejecting sets:
BF:=f(s;q;n)j9n02[0::K] : (s;q;n0)2B
Fg;(13)
B?:=f(s;q;n)j9n02[0::K] : (s;q;n0)2B
?g:(14)
We then have that P(sj=GF'F) = 18s2BF,
P(sj=GF'F) = 08s2B?andB?\F=?.
Furthermore, BFandB?are sink sets, meaning once the set
is reached, no states outside the set can be reached.
The proof of this lemma is provided in Appendix A.2. Us-
ing this lemma, we can now state and proof the main theorem
of this paper.
Theorem 2 (Optimality guarantee) .Given an LTL formula
'and a product MDP M, there exists an upper bound
U2(0;1]for rewards and a discount factor 2(0;1]
such that for all product rewards rand product discount
functionssatisfying DeÔ¨Ånition 6, the optimal determin-
istic memoryless policy rthat maximizes the expected dis-
counted reward Gr
0(s
0)is also an optimal policy 'that
maximizes the probability of satisfying the B ¬®uchi condition
P'(s
0j=GF'F)on the product MDP M.
Proof sketch. We now present a sketch of the proof to provide
intuition for the main steps and the selection of key parame-
ters. The full proof is provided in Appendix A.3.
To ensure optimality, given a policy with the product
MDPMand the LTL formula ', we want to demonstrate a
tight bound between the expected discounted reward follow-
ingand the probability of satisfying', such that maxi-
mizing one quantity is equivalent to maximizing the other.
At a high level, we want to select the two key parameters,
the reward upper bound U2(0;1]and the discount factor
2(0;1], to adequately bound: (i) the rewards given for
paths that eventually reach rejecting BSCCs (thus not satis-
fying the LTL speciÔ¨Åcation); and (ii) the discount of rewards
received from rejecting states for paths that eventually reach
accepting BSCCs.
We informally denote by C
?the expected number of visits
to accepting states before reaching a rejecting BSCC, and (i)
can be sufÔ¨Åciently bounded by selecting U= 1=C
?. Next,
we informally write C
Ffor the expected number of rejecting
states visited before reaching an accepting BSCC, and denoteAlgorithm 1: KC Q-learning from LTL
Input: environment MDP M, LTL formula '
1:translate'into an LBDAA
2:construct product MDP MusingMandA
3:initialize Q for each sand qpair
4:forl 0to max episode do
5: initialize (s;q;n) (s0;q0;0)
6: fort 0to max timestep do
7: get policyderived from Q ( e.g.,-greedy)
8: take actiona ((s;q;n))inM
9: get next product state (s0;q0;n0)
10:r r((s;q;n);a;(s0;q0;n0))
11: 0(s;q;n)
12:Q((s;q);a) r+maxa2AQ((s0;q0);a)
13: update (s;q;n) (s0;q0;n0)
14:gets greedy policy 'from Q
15:return induced policy onMby removing -actions
byNthe expected steps between visits of accepting states
in the accepting BSCC. Intuitively, for (ii), we bound the
amount of discount before reaching the accepting BSCC us-
ingC
F, and we bound the discount after reaching the BSCC
usingN, yielding= 1 1=(C
?N+C
F).
In practice, using upper bounds of C
?;C
FandNinstead
also ensures optimality, and those bounds can be deduced
from assumptions about, or knowledge of, the MDP.
As shown in the proof sketch, selecting U= 1=C
?and
= 1 1=(C
?N+C
F)is sufÔ¨Åcient to ensure optimal-
ity. Using the example of the probabilistic gate MDP in Fig-
ure 1b, we have that C
?C
F10andN= 1, so choos-
ingU= 0:1and= 0:95is sufÔ¨Åcient to guarantee opti-
mality. For more general MDPs, under the common assump-
tion that the number of states jSjand the minimum non-zero
transition probability pmin:= mins;a;s0fT(s;a;s0)>0gare
known,C
?andC
Fcan be upper bounded by jSj=pmin, while
Ncan be upper bounded by jSj.
3.4 LTL learning with Q-learning
Employing this product MDP Mand its reward structure,
we present Algorithm 1 (KC), a model-free Q-learning al-
gorithm for LTL speciÔ¨Åcations utilizing the K counter prod-
uct MDP. The product MDP is constructed on the Ô¨Çy as we
explore: for action a2A, observe the next environment
states0by taking action ain environment state s. Then, we
compute the next automaton state q0using transition func-
tion(q;L(s))and counter ndepending on whether nK
and q02F . Ifa2fqjq2Qg , update q0using the
-transition and leave environment state sand counter nun-
changed. However, directly adopting Q-learning on this prod-
uct MDP yields a Q function deÔ¨Åned on the whole product
state spaceS, meaning the agent needs to learn the Q func-
tion for each Kvalue. To improve efÔ¨Åciency, we propose
to deÔ¨Åne the Q function on the environment states Sand au-
tomaton statesQonly, and for a path ofM, the updateAlgorithm 2: CF+KC Q-learning from LTL
Input: environment MDP M, LTL formula '
1:translate'into an LBDAA
2:construct product MDP MusingMandA
3:initialize Q for each sand qpair
4:forl 0to max episode do
5: initialize (s;q;n) (s0;q0;0)
6: fort 0to max timestep do
7: get policyderived from Q (e.g., -greedy)
8: get actiona ((s;q;n))inM
9: forq2Q do
10: get counterfactual imagination (s0;q0;n0)
by taking action aat(s;q;n)
11: r r((s;q;n);a;(s0;q0;;n0))
12:  0(s;q;n)
13: Q((s;q);a) 
r+maxa2AQ((s0;q0);a)
14: obtain (s0;q0;n0)using action aat(s;q;n)
15: update (s;q;n) (s0;q0;n0)
16:gets greedy policy 'from Q
17:return induced policy onMby removing -actions
rule for the Q function at time step tis:
Qt+1((st;qt);a
t) r(s
t;a
t;s
t+1) (15)
+0(s
t) max
a2AQt((st+1;qt+1);a);
wheres
t= (st;qt;nt),a
tis the action taken at time step t,
s
t+1= (st+1;qt+1;nt+1)is the next product state, and is
the learning rate. We claim that, with this collapsed Q func-
tion, the algorithm returns the optimal policy for satisfying '
because the optimal policy is independent from the K counter,
with the proof provided in Appendix A.4.
Theorem 3. Given an environment MDP Mand an LTL
speciÔ¨Åcation 'with appropriate discount factor and re-
ward function rsatisfying Theorem 2, Q-learning for LTL
described in Algorithm 1 converges to an optimal policy '
that maximizes the probability of satisfying 'onM.
3.5 Counterfactual Imagining
Additionally, we propose a method to exploit the structure of
the product MDP, speciÔ¨Åcally the LDBA, to facilitate learn-
ing. We use counterfactual reasoning to generate synthetic
imaginations: from one state in the environment MDP, imag-
ine we are at each of the automaton states while taking the
same actions.
If the agent is at product state (s;q;n)and an action
a2Ais chosen, for each q2Q, the next state (s0;q0;n0)
by taking action afrom (s;q;n)can be computed by Ô¨Årst
taking the action in environment state, and then computing
the next automaton state q0= ( q;L(s))and the next K
valuen0. The reward for the agent is r(s;q;n), and we
can therefore update the Q function with this enriched set of
experiences. These experiences produced by counterfactualFigure 2: The experimental results on various MDPs and tasks.
imagining are still sampled from Mfollowing the transi-
tion function T, and hence, when used in conjunction with
any off-policy learning algorithms like Q-learning, the opti-
mality guarantees of the algorithm are preserved.
As shown in Algorithm 2 (CF-KC), counterfactual imag-
ining (CF) can be incorporated into our KC Q-learning algo-
rithm by altering a few lines (line 9-12 in Algorithm 2) of
code, and it can also be used in combination with other au-
tomata product MDP RL algorithms for LTL. Note that the
idea of counterfactual imagining is similar to that proposed
by Toro Icarte et al. [2022 ], but our approach has adopted
LDBAs in the product MDPs for LTL speciÔ¨Åcation learning.
4 Experimental Results
We evaluate our algorithms on various MDP environments,
including the more realistic and challenging stochastic MDP
environments1. We propose a method to directly evaluate
the probability of satisfying LTL speciÔ¨Åcations by employ-
ing probabilistic model checker PRISM [Kwiatkowska et al. ,
2011 ]. We build the induced MC Mfrom the environment
MDP and the policy in PRISM format, and adopt PRISM to
compute the exact satisfaction probability of the given LTL
speciÔ¨Åcation. We utilize tabular Q-learning as the core off-
policy learning method to implement our three algorithms:
Q-learning with Kcounter reward structure (KC), Q-learning
withKcounter reward structure and counterfactual imagin-
ing (CF+KC), and Q-learning with only counterfactual imag-
ining (CF), in which we set K= 0. We compare the per-
formance of our methods against the methods proposed by
Bozkurt et al. [2019 ], Hahn et al. [2019 ]and Hasanbeig et
al.[2020 ]. Note that our KC algorithm, in the special case
thatK= 0 with no counterfactual imagining, is algorithmi-
cally equivalent to Bozkurt et al. [2019 ]when setting their
parameterB= 1 U. Our methods differ from all other
existing methods to the best of our knowledge. The details
and setup of the experiments are given in Appendix B.
We set the learning rate = 0:1and= 0:1for explo-
ration. We also set a relatively loose upper bound on rewards
1The implementation of our algorithms and experiments can be
found on GitHub: https://github.com/shaodaqian/rl-from-ltlU= 0:1and discount factor = 0:99for all experiments to
ensure optimality. Note that the optimality of our algorithms
holds for a family of reward structures deÔ¨Åned in DeÔ¨Ånition 6,
and for experiments we opt for a speciÔ¨Åc reward function that
linearly increases the reward for accepting states as the value
ofKincreases, namely rn=Un=K8n2[0::K], to facili-
tate training and exploration. The Q function is optimistically
initialized by setting the Q value for all available state-action
pairs to 2U. All experiments are run 100 times, where we plot
the average satisfaction probability with half standard devia-
tion in the shaded area.
First, we conduct experiments on the probabilistic gate
MDP described in Example 1 with task ‚Äú FGa &G!c‚Äù,
which means reaching only states labeled ‚Äúa‚Äù in the future
while never reaching ‚Äúc‚Äù labeled states. We set K= 10
for this task, and in Figure 2 (left), compared to the other
three methods, our method KC achieved better sample efÔ¨Å-
ciency and convergence and CF demonstrates better sample
efÔ¨Åciency while still lacking training stability. The best per-
formance is achieved by CF+KC, while other methods either
exhibit slower convergence (Bozkurt et al. [2019 ]and Hahn
et al. [2019 ]) or fail to converge (Hasanbeig et al. [2020 ]) due
to the lack of theoretical optimality guarantees.
The second MDP environment is the 88frozen lake en-
vironment from OpenAI Gym [Brockman et al. , 2016 ]. This
environment consists of frozen lake tiles, where the agent has
1/3 chance of moving in the intended direction and 1/3 of go-
ing sideways each, with details provided in Appendix B.2.
The task is ‚Äú( GFajGFb) &G!h‚Äù, meaning to always reach
lake camp ‚Äúa‚Äù or lake camp ‚Äúb‚Äù while never falling into holes
‚Äúh‚Äù. We set K= 10 for this task, and in Figure 2 (middle),
we observe signiÔ¨Åcantly better sample efÔ¨Åciency for all our
methods, especially for CF+KC and CF, which converge to
the optimal policy at around 150k training steps. The other
three methods, on the other hand, barely start to converge at
1200k training steps. CF performs especially well in this task
because the choice of always reaching ‚Äúa‚Äù or ‚Äúb‚Äù can be con-
sidered simultaneously during each time step, reducing the
sample complexity to explore the environment.
Lastly, we experiment on a slight modiÔ¨Åcation of theFigure 3: Sensitivity analysis on key parameters.
more challenging ofÔ¨Åce world environment proposed by Toro
Icarte et al. [2022 ], with details provided in Appendix B.3.
We include patches of icy surfaces in the ofÔ¨Åce world, with
the task to either patrol in the corridor between ‚Äúa‚Äù and ‚Äúb‚Äù,
or write letters at ‚Äúl‚Äù and then patrol between getting tea ‚Äút‚Äù
and workplace ‚Äúw‚Äù, while never hitting obstacles ‚Äúo‚Äù. K= 5
is set for this task for a steeper increase in reward, since the
long distance between patrolling states makes visiting many
accepting states in each episode time consuming. Figure 2
(right) presents again the performance beneÔ¨Åt of our meth-
ods, with CF+KC performing the best and CF and KC second
and third, respectively. For this challenging task in a large
environment, the method of Hahn et al. [2019 ]requires the
highest number of training steps to converge.
Overall, the results demonstrate KC improves both sam-
ple efÔ¨Åciency and training stability, especially for challeng-
ing tasks. In addition, CF greatly improves sample efÔ¨Åciency,
which, combined with KC, achieves the best results.
4.1 Runtime analysis and sensitivity analysis
It is worth mentioning that, for counterfactual imagining,
multiple updates to the Q function are performed at each step
in the environment. This increases the computational com-
plexity, but the additional inner loop updates on the Q func-
tion will only marginally affect the overall computation time
if the environment steps are computationally expensive. Tak-
ing the ofÔ¨Åce world task as an example, the average time to
perform 6 million training steps are 170.9s, 206.3s and 236.1s
for KC, CF and CF+KC, respectively. However, the time un-
til convergence to the optimal policies are 96.8s, 70.2s and
27.5s for KC, CF and CF+KC, respectively.
For sensitivity analysis on the key parameters, we run ex-
periments on the probabilistic gate MDP task with different
parameters against the default values of U= 0:1;= 0:99
andK= 10 . As shown in Figure 3, if U(= 0:5)is chosen
too high or(= 0:9)is chosen too low, the algorithm does not
converge to the optimal policy as expected. However, looser
parametersU= 0:01and= 0:995do not harm the per-
formance, which means that, even with limited knowledge of
the underlying MDP, our algorithm still performs well with
loose parameters. Optimality is not affected by the Kvalue,
while the performance is only mildly affected by different K
values.5 Conclusion
We presented a novel model-free reinforcement learning al-
gorithm to learn the optimal policy of satisfying LTL speciÔ¨Å-
cations in an unknown stochastic MDP with optimality guar-
antees. We proposed a novel product MDP, a generalized re-
ward structure and a RL algorithm that ensures convergence
to the optimal policy with the appropriate parameters. Fur-
thermore, we incorporated counterfactual imagining, which
exploits the LTL speciÔ¨Åcation to create imagination experi-
ences. Lastly, utilizing PRISM [Kwiatkowska et al. , 2011 ],
we directly evaluated the performance of our methods and
demonstrated superior performance on various MDP environ-
ments and LTL tasks.
Future works include exploring other speciÔ¨Åc reward func-
tions under our generalized reward structure framework, tack-
ling the open problem [Alur et al. , 2021 ]of dropping all as-
sumptions regarding the underlying MDP, and extending the
theoretical framework to continuous states and action envi-
ronment MDPs, which might be addressed through an ab-
straction of the state space. In addition, utilizing potential-
based reward shaping [Nget al. , 1999 ][Devlin and Kudenko,
2012 ]to exploit the semantic class structures of LTL speciÔ¨Å-
cations and transferring similar temporal logic knowledge of
the agent [Xu and Topcu, 2019 ]between environments could
also be interesting.
Acknowledgments
This work was supported by the EPSRC Prosperity Partner-
ship FAIR (grant number EP/V056883/1). DS acknowledges
funding from the Turing Institute and Accenture collabora-
tion. MK receives funding from the ERC under the European
Union‚Äôs Horizon 2020 research and innovation programme
(FUN2MODEL, grant agreement No. 834115).
References
[Aksaray et al. , 2016 ]Derya Aksaray, Austin Jones, Zhao-
dan Kong, Mac Schwager, and Calin Belta. Q-learning
for robust satisfaction of signal temporal logic speciÔ¨Åca-
tions. 2016 IEEE 55th Conference on Decision and Con-
trol, CDC 2016 , pages 6565‚Äì6570, 9 2016.
[Alur et al. , 2021 ]Rajeev Alur, Suguman Bansal, Osbert
Bastani, and Kishor Jothimurugan. A framework for
transforming speciÔ¨Åcations in reinforcement learning.
arXiv:2111.00272 , 2021.
[Andreas et al. , 2016 ]Jacob Andreas, Dan Klein, and
Sergey Levine. Modular multitask reinforcement learn-
ing with policy sketches. 34th International Conference
on Machine Learning, ICML 2017 , 1:229‚Äì239, 11 2016.
[Baier and Katoen, 2008 ]Christel Baier and Joost-Pieter
Katoen. Principles Of Model Checking , volume 950. MIT
Press, 2008.
[Bozkurt et al. , 2019 ]Alper Kamil Bozkurt, Yu Wang,
Michael M. Zavlanos, and Miroslav Pajic. Control synthe-
sis from linear temporal logic speciÔ¨Åcations using model-
free reinforcement learning. Proceedings - IEEE Inter-
national Conference on Robotics and Automation , pages
10349‚Äì10355, 2019.[Brockman et al. , 2016 ]Greg Brockman, Vicki Cheung,
Ludwig Pettersson, Jonas Schneider, John Schulman, Jie
Tang, and Wojciech Zaremba Openai. OpenAI Gym.
arXiv:1606.01540 , 6 2016.
[Caiet al. , 2021 ]Mingyu Cai, Mohammadhosein Hasan-
beig, Shaoping Xiao, Alessandro Abate, and Zhen Kan.
Modular deep reinforcement learning for continuous mo-
tion planning with temporal logic. IEEE Robotics and Au-
tomation Letters , 6(4):7973‚Äì7980, 2021.
[Camacho et al. , 2019 ]Alberto Camacho, Rodrigo
Toro Icarte, Toryn Q. Klassen, Richard Valenzano,
and Sheila A. McIlraith. LTL and beyond: Formal
languages for reward function speciÔ¨Åcation in reinforce-
ment learning. IJCAI International Joint Conference on
ArtiÔ¨Åcial Intelligence , 2019-August:6065‚Äì6073, 2019.
[De Giacomo and Vardi, 2013 ]Giuseppe De Giacomo and
Moshe Y Vardi. Linear temporal logic and linear dynamic
logic on Ô¨Ånite traces. Proceedings of the AAAI Conference
on ArtiÔ¨Åcial Intelligence , 2013.
[Devlin and Kudenko, 2012 ]Sam Devlin and Daniel Ku-
denko. Dynamic potential-based reward shaping. Proceed-
ings of the 11th International Conference on Autonomous
Agents and Multiagent Systems , 2012.
[Giuseppe De Giacomo et al. , 2019 ]Giuseppe De Giacomo,
Luca Iocchi, Marco Favorito, and Fabio Patrizi. Foun-
dations for restraining bolts: reinforcement learning with
LTLf/LDLf restraining speciÔ¨Åcations. In Proceedings of
the Twenty-Ninth International Conference on Automated
Planning and Scheduling , 2019.
[Hahn et al. , 2019 ]Ernst Moritz Hahn, Mateo Perez, Sven
Schewe, Fabio Somenzi, Ashutosh Trivedi, and Dominik
Wojtczak. Omega-regular objectives in model-free rein-
forcement learning. Proceedings of the International Con-
ference on Tools and Algorithms for the Construction and
Analysis of Systems , 11427 LNCS:395‚Äì412, 2019.
[Hahn et al. , 2020 ]Ernst Moritz Hahn, Mateo Perez, Sven
Schewe, Fabio Somenzi, Ashutosh Trivedi, and Dominik
Wojtczak. Faithful and effective reward schemes for
model-free reinforcement learning of omega-regular ob-
jectives. Automated Technology for VeriÔ¨Åcation and Anal-
ysis, 12302 LNCS:108‚Äì124, 2020.
[Hasanbeig et al. , 2019 ]M. Hasanbeig, Y . Kantaros,
A. Abate, D. Kroening, G. J. Pappas, and I. Lee. Rein-
forcement learning for temporal logic control synthesis
with probabilistic satisfaction guarantees. Proceed-
ings of the IEEE Conference on Decision and Control ,
2019-December:5338‚Äì5343, 9 2019.
[Hasanbeig et al. , 2020 ]Mohammadhosein Hasanbeig,
Daniel Kroening, and Alessandro Abate. Deep rein-
forcement learning with temporal logics. Proceedings of
the International Conference on Formal Modeling and
Analysis of Timed Systems , 12288 LNCS:1‚Äì22, 2020.
[Jiang et al. , 2021 ]Yuqian Jiang, Suda Bharadwaj, Bo Wu,
Rishi Shah, Ufuk Topcu, and Peter Stone. Temporal-logic-
based reward shaping for continuing reinforcement learn-ing tasks. Proceedings of the AAAI Conference on ArtiÔ¨Å-
cial Intelligence , 35(9):7995‚Äì8003, 5 2021.
[Jothimurugan et al. , 2020 ]Kishor Jothimurugan, Rajeev
Alur, and Osbert Bastani. A composable speciÔ¨Åcation lan-
guage for reinforcement learning tasks. Advances in Neu-
ral Information Processing Systems , 32, 8 2020.
[Jothimurugan et al. , 2021 ]Kishor Jothimurugan, Suguman
Bansal, Osbert Bastani, and Rajeev Alur. Compo-
sitional reinforcement learning from logical speciÔ¨Åca-
tions. Advances in Neural Information Processing Sys-
tems, 12:10026‚Äì10039, 6 2021.
[KÀáret¬¥ƒ±nsk¬¥yet al. , 2018 ]Jan K Àáret¬¥ƒ±nsk¬¥y, Tobias Meggendor-
fer, Salomon Sickert, and Christopher Ziegler. Rabinizer
4: From LTL to your favourite deterministic automaton.
Proceedings of the International Conference on Computer
Aided VeriÔ¨Åcation , 10981 LNCS:567‚Äì577, 2018.
[Kwiatkowska et al. , 2011 ]Marta Kwiatkowska, Gethin
Norman, and David Parker. PRISM 4.0: VeriÔ¨Åcation
of probabilistic real-time systems. Computer Aided
VeriÔ¨Åcation , 6806 LNCS:585‚Äì591, 2011.
[Liet al. , 2016 ]Xiao Li, Cristian Ioan Vasile, and Calin
Belta. Reinforcement learning with temporal logic re-
wards. IEEE International Conference on Intelligent
Robots and Systems , 2017-September:3834‚Äì3839, 12
2016.
[Littman et al. , 2017 ]Michael L. Littman, Ufuk Topcu, Jie
Fu, Charles Isbell, Min Wen, and James MacGlashan.
Environment-independent task speciÔ¨Åcations via GLTL.
arXiv:1704.04341 , 4 2017.
[Littman, 2001 ]M.L. Littman. Markov decision processes.
International Encyclopedia of the Social & Behavioral
Sciences , pages 9240‚Äì9242, 2001.
[Majeed and Hutter, 2018 ]Sultan Javed Majeed and Marcus
Hutter. On q-learning convergence for non-markov deci-
sion processes. IJCAI International Joint Conference on
ArtiÔ¨Åcial Intelligence , 2018-July:2546‚Äì2552, 2018.
[Nget al. , 1999 ]Andrew Y . Ng, Andrew Y . Ng, Daishi
Harada, and Stuart Russell. Policy invariance under re-
ward transformations: Theory and application to reward
shaping. Proceedings of the Sixteenth International Con-
ference on Machine Learning , pages 278‚Äì287, 1999.
[Sadigh et al. , 2014 ]Dorsa Sadigh, Eric S. Kim, Samuel
Coogan, S. Shankar Sastry, and Sanjit A. Seshia. A learn-
ing based approach to control synthesis of Markov de-
cision processes for linear temporal logic speciÔ¨Åcations.
Proceedings of the IEEE Conference on Decision and
Control , 2015-February(February):1091‚Äì1096, 2014.
[Sickert et al. , 2016 ]Salomon Sickert, Javier Esparza, Ste-
fan Jaax, and Jan K Àáret¬¥ƒ±nsk¬¥y. Limit-deterministic B ¬®uchi
automata for linear temporal logic. Computer Aided Veri-
Ô¨Åcation , 9780:312‚Äì332, 2016.
[Sutton and Barto, 2018 ]Richard S Sutton and Andrew G
Barto. Reinforcement learning: An Introduction , vol-
ume 3. MIT Press, 2018.[Toro Icarte et al. , 2018 ]Rodrigo Toro Icarte, Toryn Q
Klassen, Richard Valenzano, and Sheila A Mcllraith. Us-
ing reward machines for high-level task speciÔ¨Åcation and
decomposition in reinforcement learning. Proceedings of
the 35th International Conference on Machine Learning ,
2018.
[Toro Icarte et al. , 2022 ]Rodrigo Toro Icarte, Toryn Q.
Klassen, Richard Valenzano, and Sheila A. McIlraith. Re-
ward machines: Exploiting reward function structure in
reinforcement learning. Journal of ArtiÔ¨Åcial Intelligence
Research , 73:173‚Äì208, 2022.
[Watkins and Dayan, 1992 ]Christopher J. C. H. Watkins and
Peter Dayan. Q-learning. Machine Learning 1992 8:3 ,
8(3):279‚Äì292, 5 1992.
[Xu and Topcu, 2019 ]Zhe Xu and Ufuk Topcu. Transfer of
temporal logic formulas in reinforcement learning. Pro-
ceedings of the Twenty-Eighth International Joint Confer-
ence on ArtiÔ¨Åcial Intelligence , 2019.A Theoretical Results: Proofs
A.1 Proof of Theorem 1
Proof. We will prove the equality by verifying both sides of the inequality and subsequently constructing the induced policy
onM.
For, it has been proven [Sickert et al. , 2016 ]that, for every accepting path of the environment MDP Mfollowing a
policy, there always exists a corresponding accepting run of the LDBA that resolves the non-determinism. Augmenting the
resolved non-determinism as -actions to the original path yields an accepting path of M, since the counter will eventually
reachKand the accepting states Fwill be visited inÔ¨Ånitely often. Therefore, we have created a policy for Mthat is at least
as good asonM.
For, it is clear that any policy onMcan induce a policy for Mby eliminating the -transitions and removing the
projection of the automaton and Kcounter. Therefore, any path following that meets the B ¬®uchi condition 'Fwill induce
a path ofMthat is accepting by Ainduced from ', where the non-determinism of Ais resolved by -transitions of , thus
satisfying'.
A.2 Proof of Lemma 1
Proof. To begin with, we recall the property of BSCCs in Markov chains (MC): for any inÔ¨Ånite path of a MC, a BSCC
will eventually be reached, and once reached it can‚Äôt reach any state outside the BSCC and all states within it will be visited
inÔ¨Ånitely often with probability 1. Therefore, since all s2B
Fbelongs to BSCCs with accepting states, all paths from s
will reach accepting states inÔ¨Ånitely often with probability 1, so P(sj=GF'F) = 18s2B
F. Similarly, all s2B
?
belong to BSCCs with no accepting states, so P(sj=GF'F) = 08s2B
?.
Now, we observe that, for states in M
, the Markov chain transitions for environment states and automaton states are in fact
independent of the counter value:
T
((s;q;n1);(s0;q0;min(n1+ 1;K))) =T
((s;q;n2);(s0;q0;min(n2+ 1;K))) (16)
for alln1;n22[0::K];s;s02Sand q;q02Q. With this observation, for all (s;q;n)2BF, there exists (s;q;n0)2B
F,
which belongs to some BSCC by Line 13. Therefore, any state (s1;q1;n1)reachable from (s;q;n)by Equation 16 implies
there existsn2such that (s1;q1;n2)is reachable from (s;q;n0). This, by the deÔ¨Ånition of BSCC, means (s1;q1;n2)2B
F,
which implies (s1;q1;n1)2BFby Line 13, which shows that BFis a sink set. Furthermore, since (s;q;n0)2B
Fbelongs to
an accepting BSCC, it can reach an accepting state (sF;qF;nF)2B
Fin that BSCC with probability 1. By Equation 16, there
exists (sF;qF;n3)2BFreachable from (s;q;n)with probability 1 and, by the accepting states of DeÔ¨Ånition 5 of product
MDP, (sF;qF;n3)is also accepting, meaning accepting states can be reached from any state in BFwith probability 1. Together
with the fact that BFis a sink set, we conclude that accepting states can be reached inÔ¨Ånitely often with probability 1 from BF,
meaning P(sj=GF'F) = 18s2BF. Another observation is that, since the counter values are increasing for all paths
and accepting states will be reached inÔ¨Ånitely often in accepting BSCCs, all states in accepting BSCCs must have counter value
equal toK.
Using the same argument, we conclude that B?is also a sink set. In addition, for (s;q;n)2B?, there exists (s;q;n0)2B
?
which belongs to a rejecting BSCC such that no accepting states can be reached from it. By Equation 16 and the accepting states
deÔ¨Ånition of product MDP (DeÔ¨Ånition 5), we see that no accepting state can be reached from (s;q;n)either, which implies
P(sj=GF'F) = 08s2B?andB?\F=?, which completes the proof.
A.3 Proof of Theorem 2
Proof. To give an outline of the proof, we would like to show that, for any deterministic memoryless policy , the expected
discounted reward G
0(s)for all product states with counter value 0, i.e.s= (s;q;0), is close to the probability P(sj=
GF'F)of satisfying 'Fstarting from sfollowing. We show this by upper and lower bounding the difference between the
two quantities.
Froms= (s;q;0), we consider the expected discounted reward conditioned on whether the inÔ¨Ånite path following policy
satisÔ¨Åes the B ¬®uchi condition 'For not.
G
0(s) =G
0(sjsj=GF'F)P(sj=GF'F) (17)
+G
0(sjs6j=GF'F)P(s6j=GF'F) (18)
We Ô¨Årst consider the components of Line 17. Let the stopping time of Ô¨Årst reaching the accepting set BFfromsbe

F:= infft >0js
t2BFgand let the Ô¨Årst reached state in BFbe(s;q;n). Then, let the hitting time between accepting
states in the accepting BSCC Bcontaining (s;q;n)2B
Fbe
BSCC . Furthermore, let c
Facc:=t
F:s
t2Fandc
Frej:=t
F:s
t=2Fbe the number of accepting and non-accepting states reached before reaching BFfromsre-
spectively, where c
Facc+c
Frej=
F. Note that all the quantities above depend on the starting state sbut for convenience
we omit it in the notations. Then, we have
G
0(sjsj=GF'F) (19)
=E2
41X
i=0(i 1Y
j=0(s
j))r
is
0=sj=GF'Fj3
5 (20)
E2
4c
Frej0
@c
FaccX
n=0(n 1Y
j=0(1 Rj))Rn1
A+1X
i=
F(i 1Y
j=0(s
j))r
is
0=sj=GF'F)3
5 (21)
E2
4c
Frej0
@c
FaccX
n=0(n 1Y
j=0(1 Rj))Rn+1X
i=
F(c
FaccY
j=0(1 Rj)i 1Y
k=
F(s
k))r
i1
As
0=sj=GF'F)3
5 (22)
E2
4c
Frej0
@c
FaccX
n=0(n 1Y
j=0(1 Rj))Rn+
BSCC=U1X
n=c
Facc(n 1Y
j=0(1 Rj))Rn1
As
0=sj=GF'F)3
5 (23)
C
F+N=U0
@1X
n=0(n 1Y
j=0(1 Rj))Rn1
A (24)
=C
F+N=U1 (25)
whereC
F=E[c
Frejjs
0=sj=GF'F)],N=E[
BSCCjs
0=sj=GF'F)],Rnis the non-zero rewards in
DeÔ¨Ånition 6 and we slightly abuse the notation to let Rn=RK8n>K . The inequality in Line 21 holds because, for the Ô¨Årst

Felements in the sum, there are at most c
Frejzero reward discount terms in the product and taking them out of the sum
leaves only the non-zero reward terms. Line 22 holds similarly by Ô¨Årst taking c
Frejdiscountterms out of the product for
the rest of the sum, leaving the (1 Rj)terms and the rest of the discount factors (s
k). Line 23 holds by Lemma 1 and the
observation that, after reaching the accepting BSCC, each non-zero reward will receive 
BSCC additional discount between
accepting states. In addition, by summing the inÔ¨Ånite sequences we Ô¨Ånd U=(1 
BSCC (1 U))
BSCC=Ufor2(0;1)
andU2(0;1), upper bounding the additional discount and leaving only the non-zero reward discount factors 1 Rj. Finally,
Line 25 holds by induction because the inÔ¨Ånite geometric sumP1
n=0(1 RK)nRK= 1andRn+ (1 Rn)1 = 1 for all
n<K .
Intuitively, the expected discounted reward for paths with only accepting states is 1, and we lower bound the expected
discounted reward for general paths satisfying 'Fby bounding the amount of discount the reward receives from non-zero
reward states.
Next, we consider the components of Line 18. Let the stopping time of Ô¨Årst reaching the rejecting set B?fromsbe

?:= infft>0js
t2B?gand letc
?acc:=t
?:s
t2Fbe the number of accepting states reached before reaching
B?froms. We similarly omit the dependency on sin the notation. Then, we have
G
0(sjs6j=GF'F) =E2
41X
i=0(i 1Y
j=0(s
j))r
is
0=s6j=GF'F)3
5 (26)
E2
4c
?accX
n=0(n 1Y
j=0(1 Rj))Rns
0=s6j=GF'F)3
5 (27)
=C
?X
n=0(n 1Y
j=0(1 Rj))Rn (28)
C
?X
n=0(1 U)nU (29)
= 1 (1 U)C
? (30)whereC
?=E[c
?accjs
0=s6j=GF'F]. In Line 27, the inequality holds because, by Lemma 1, once B?is reached,
no further accepting states with non-zero rewards can be reached and the total reward can be bounded above by omitting the
discount factor from non-accepting states and summing the discounted reward only for the c
?accaccepting states reached
beforeB?. Line 28 holds by taking expectation on 
?and Line 29 holds by induction because Rn+ (1 Rn)X1
U+ (1 U)X2for alln2[0::K]ifX1X2, andURnis an upper bound for all rewards. Intuitively, we have bounded
the amount of non-zero reward received by paths not satisfying 'F.
Therefore, from Equation 17 and these technical results, we have a lower bound for the expected discounted reward
G
0(s)C
F+N=UP(sj=GF'F) (31)
by assuming G
0(sjs6j=GF'F) = 0 , and an upper bound of the expected discounted reward:
G
0(s)P(sj=GF'F) + (1 (1 U)C
?)P(s6j=GF'F) (32)
=P(sj=GF'F) + (1 (1 U)C
?)(1 P(sj=GF'F)) (33)
= 1 (1 U)C
?+P(sj=GF'F)(1 U)C
? (34)
Last but not least, since deterministic memoryless policy is considered in a Ô¨Ånite MDP, there is a Ô¨Ånite set of policies and we
let the difference in probability of satisfying 'Fbetween the optimal policy and the best sub-optimal policy be :=P'(s
0j=
GF'F) maxf6='gP(s
0j=GF'F). We can let the reward upper bound U2(0;1)be small enough and 2(0;1)
large enough such that following the bounds of Line 31 and Line 34, we have that
G
0(s)max
6='fP(s
0j=GF'F)g+=P'(s
0j=GF'F)G'
0(s)86=' (35)
This means the expected discounted reward for the optimal policy 'must be greater than the expected discounted reward
received by any sub-optimal policy. We can therefore conclude that the policy rmaximizes the total expected discounted
rewardGr
0(s)is also the optimal policy 'that maximizes the probability of satisfying the B ¬®uchi condition 'Fon the
product MDPM, which completes the proof.
For choosing the key parameters U2(0;1)and2(0;1)to ensure optimality, if we assume a reasonable gap 0:5
between the optimal and sub-optimal policies, it generally sufÔ¨Åces to let U= 1=C
?and= 1 1=(C
?N+C
F). For
example, with the example probabilistic gate MDP in Figure 1b, we have C
?C
F10andN= 1, so choosing U= 0:1
and= 0:95is sufÔ¨Åcient. If the hitting times and stopping times are not obtainable, under the common assumption for MDPs
that the number of states jSjinMand the minimum non-zero transition probability pmin:= mins;a;s0fT(s;a;s0)>0gare
known,C
?andC
Fcan be upper bounded by jSj=pminandNcan be upper bounded by jSj.
A.4 Proof of theorem 3
Proof. To begin with, recall from Theorem 1 and Theorem 2 that the policy rmaximizing the expected discounted reward of
Minduces the optimal policy for satisfying 'inMby removing the -actions.
Next, note that, despite the reward for Q-learning in Algorithm 1 being non-Markovian, following the proof of convergence
to optimal Q value for non-Markovian Q-learning [Majeed and Hutter, 2018 ], with theKcounter in DeÔ¨Ånition 5 and the reward
in DeÔ¨Ånition 6, we verify that the Q function in Algorithm 1 converges to the optimal Q value for all (s;q)and actiona,
which is the expected discounted reward by taking action afrom (s;q;0).
With this, recall that the environment states and automaton states transitions do not depend on the counter value by Equa-
tion 16 and the non-zero reward is the same for accepting states with the same counter value by DeÔ¨Ånition 6. We argue that, for
the optimal discounted reward policy r, the new policy 0((s;q;n)) =r((s;q;0))8n2[0::K], where the policy for each
counter values is set to be the same, remains optimal. This is because, if 0is sub-optimal, meaning the satisÔ¨Åability of 'Fis
lower than that of the optimal policy, it must be that 0makes a wrong decision at some counter value such that the reachability
probability to some accepting BSCC Bis sub-optimal. This means rmust also be sub-optimal because when counter value
equals 0, by taking the same actions as 0, the reachability probability to the BSCC Bis sub-optimal, yielding a contradiction.
Lastly, since 0is independent from the counter values, the policy derived from the collapsed optimal Q function learned by
Algorithm 1 is the same as the optimal 0by ignoring the Kcounter, meaning that Algorithm 1 converges to the optimal policy,
completing the proof.
B Additional Experimental Details And Setup
Experiments are carried out on a Linux server (Ubuntu 18.04.2) with two Intel Xeon Gold 6252 CPUs and six NVIDIA GeForce
RTX 2080 Ti GPUs. All our algorithms are implemented in Python, and the full source code can be found in our supplementary
material. We select three stochastic environments (probabilistic gate, frozen lake and ofÔ¨Åce world) described below with several
difÔ¨Åcult LTL tasks. For all environments and tasks we set the learning rate = 0:1and the exploration rate = 0:1, with the
upper bound on rewards U= 0:1and discount factor = 0:99to ensure optimality. For the method proposed by Hahn et
al.[2019 ], we adopt their implementation from the Mungojerrie tool2. We set the default parameter values to be = 0:1and
2https://plv.colorado.edu/wwwmungojerrie/docs/v1 0/index.htmlFigure 4: The optimal policy for the example probabilistic gate MDP task1
(a) The MDP environment for the frozen lake task. Blue rep-
resents ice, ‚Äúh‚Äù are holes, ‚Äúa‚Äù and ‚Äúb‚Äù are lake camps, and the
purple triangle is the start.
(b) The MDP environment for the ofÔ¨Åce world task. Blue represents
ice, ‚Äúo‚Äù are obstacles, ‚Äúw‚Äù, ‚Äúl‚Äù and ‚Äút‚Äù represent workplace, letter and
tea respectively, and the purple triangle is the start.
Figure 5: MDP environments used in the experiments.
= 0:1, with= 0:995for the probabilistic gate task and = 0:99for the frozen lake and ofÔ¨Åce world tasks. For the method
by Hasanbeig et al. [2020 ], we use the implementation in their GitHub repository3with= 0:99,= 0:1and= 0:1. For
the method by Bozkurt et al. [2019 ], we also adopt their GitHub repository4with= 0:99,B= 0:9,= 0:1and= 0:1.
For the environments and tasks used for experimentation, we speciÔ¨Åcally choose commonly used environments with stochas-
tic transitions that are more realistic and challenging, especially for inÔ¨Ånite-horizon LTL tasks. The LTL tasks we use are
standard task speciÔ¨Åcations for robotics and autonomous systems. The tasks for the frozen lake and the ofÔ¨Åce world environ-
ments are more challenging because they involve letting the agent choose between two subtasks, which better showcase the
beneÔ¨Åts of the Kcounter and counterfactual imagining.
B.1 Probabilistic Gate
The probabilistic gate MDP is described in Example 1, where the task is to visit states labeled ‚Äúa‚Äù inÔ¨Ånitely often without
visiting states labeled ‚Äúc‚Äù. We set the episode length to be 100 (which is the maximum length of a path in the MDP) and the
number of episodes to be 40000. In Figure 4, the optimal policy is demonstrated by the black arrows, where the agent goes
down from the start and before going all the way to the right to (2,9), which is an accepting sink state.
B.2 Frozen Lake
The frozen lake [Brockman et al. , 2016 ]environment is shown in Figure 5a. The blue states represent the frozen lake, where
the agent has 1/3 probability of moving in the intended direction and 1/3 each of going sideways (left or right). The white states
with label ‚Äúh‚Äù are holes and states with label ‚Äúa‚Äù and ‚Äúb‚Äù are lake camps. The task is ‚Äú( GFajGFb) &G!h‚Äù, meaning to
always reach lake camp ‚Äúa‚Äù or lake camp ‚Äúb‚Äù while never falling into holes ‚Äúh‚Äù. For this task, we set the episode length to be
200 and the number of episodes to be 6000.
3https://github.com/grockious/lcrl
4https://github.com/alperkamil/csrlB.3 OfÔ¨Åce World
The ofÔ¨Åce world [Toro Icarte et al. , 2022 ]environment is demonstrated in Figure 5b. We include a patch of ice labeled in blue
as deÔ¨Åned in the frozen lake environment and two one-directional gates at (1;3)and(9;3), where the agent can only cross to
the right as shown by the direction of the gray triangle. The gray blocks are walls, the states labeled ‚Äúo‚Äù, ‚Äúl‚Äù, ‚Äút‚Äù and ‚Äúw‚Äù are
obstacles, letters, tea and the workplace respectively. The task for this environment is also demanding: ‚Äú( GFa &GFb)j(F
l &X(GFt &GFw)) & G!o‚Äù. This means to either patrol in the corridor between ‚Äúa‚Äù and ‚Äúb‚Äù, or go to write letter at ‚Äúl‚Äù
and then patrol between getting tea ‚Äút‚Äù and workplace ‚Äúw‚Äù, whilst never hitting obstacles ‚Äúo‚Äù. For this task, we set the episode
length to be 1000 and the number of episodes to be 6000.
B.4 PRISM
We use PRISM version 4.7 [Kwiatkowska et al. , 2011 ]to evaluate the learned policy against the LTL tasks. The PRISM tool and
the installation documentations can be obtained from the their ofÔ¨Åcial website5. Each time we would like to evaluate the policy
on the environment MDP, our Python code autonomously constructs the induced Markov chain from the MDP and the policy
as a discrete-time Markov chain in the PRISM language, and evaluates this PRISM model against the LTL task speciÔ¨Åcation
through a call from Python. The max iteration parameter for PRISM is set to 100000, and we evaluate the current policy every
10000 training steps for plotting the training graph for all our experiments.
Example PRISM model
We provide an example PRISM model of the induced Markov chain from the probabilistic gate MDP and its optimal policy.
1dtmc
2module ProductMDP
3 m : [ 0 . . 4 0 ] i n i t 1 0 ;
4 a : [ 0 . . 3 ] i n i t 0 ;
5 [ ep ] (m=0) &(a =0) ‚àí >(m‚Äô =0) &(a ‚Äô =1) ;
6 [ ac ] (m=0) &(a =1) ‚àí >1 : (m‚Äô =1) &(a ‚Äô =1) ;
7 [ ac ] (m=0) &(a =2) ‚àí >1 : (m‚Äô =1) &(a ‚Äô =2) ;
8 [ ep ] (m=1) &(a =0) ‚àí >(m‚Äô =1) &(a ‚Äô =1) ;
9 [ ac ] (m=1) &(a =1) ‚àí >1 : (m‚Äô =2) &(a ‚Äô =1) ;
10 [ ac ] (m=1) &(a =2) ‚àí >1 : (m‚Äô =2) &(a ‚Äô =2) ;
11 [ ep ] (m=2) &(a =0) ‚àí >(m‚Äô =2) &(a ‚Äô =1) ;
12 [ ac ] (m=2) &(a =1) ‚àí >1 : (m‚Äô =3) &(a ‚Äô =1) ;
13 [ ac ] (m=2) &(a =2) ‚àí >1 : (m‚Äô =3) &(a ‚Äô =2) ;
14 [ ep ] (m=3) &(a =0) ‚àí >(m‚Äô =3) &(a ‚Äô =1) ;
15 [ ac ] (m=3) &(a =1) ‚àí >1 : (m‚Äô =4) &(a ‚Äô =1) ;
16 [ ac ] (m=3) &(a =2) ‚àí >1 : (m‚Äô =4) &(a ‚Äô =2) ;
17 [ ep ] (m=4) &(a =0) ‚àí >(m‚Äô =4) &(a ‚Äô =1) ;
18 [ ac ] (m=4) &(a =1) ‚àí >1 : (m‚Äô =5) &(a ‚Äô =1) ;
19 [ ac ] (m=4) &(a =2) ‚àí >1 : (m‚Äô =5) &(a ‚Äô =2) ;
20 [ ep ] (m=5) &(a =0) ‚àí >(m‚Äô =5) &(a ‚Äô =1) ;
21 [ ac ] (m=5) &(a =1) ‚àí >1 : (m‚Äô =6) &(a ‚Äô =1) ;
22 [ ac ] (m=5) &(a =2) ‚àí >1 : (m‚Äô =6) &(a ‚Äô =2) ;
23 [ ep ] (m=6) &(a =0) ‚àí >(m‚Äô =6) &(a ‚Äô =1) ;
24 [ ac ] (m=6) &(a =1) ‚àí >1 : (m‚Äô =7) &(a ‚Äô =1) ;
25 [ ac ] (m=6) &(a =2) ‚àí >1 : (m‚Äô =7) &(a ‚Äô =2) ;
26 [ ep ] (m=7) &(a =0) ‚àí >(m‚Äô =7) &(a ‚Äô =1) ;
27 [ ac ] (m=7) &(a =1) ‚àí >1 : (m‚Äô =8) &(a ‚Äô =1) ;
28 [ ac ] (m=7) &(a =2) ‚àí >1 : (m‚Äô =8) &(a ‚Äô =2) ;
29 [ ep ] (m=8) &(a =0) ‚àí >(m‚Äô =8) &(a ‚Äô =1) ;
30 [ ac ] (m=8) &(a =1) ‚àí >1 : (m‚Äô =9) &(a ‚Äô =1) ;
31 [ ac ] (m=8) &(a =2) ‚àí >1 : (m‚Äô =9) &(a ‚Äô =2) ;
32 [ ep ] (m=9) &(a =0) ‚àí >(m‚Äô =9) &(a ‚Äô =1) ;
33 [ ac ] (m=9) &(a =1) ‚àí >1 . 0 : (m‚Äô =9) &(a ‚Äô =2) ;
34 [ ac ] (m=9) &(a =2) ‚àí >1 . 0 : (m‚Äô =9) &(a ‚Äô =2) ;
35 [ ac ] (m=10) &(a =0) ‚àí >1 : (m‚Äô =20) &(a ‚Äô =0) ;
36 [ ac ] (m=10) &(a =1) ‚àí >1 : (m‚Äô =20) &(a ‚Äô =2) ;
37 [ ac ] (m=10) &(a =2) ‚àí >1 : (m‚Äô =20) &(a ‚Äô =2) ;
38 [ ac ] (m=11) &(a =0) ‚àí >1 . 0 : (m‚Äô =11) &(a ‚Äô =0) ;
39 [ ac ] (m=11) &(a =1) ‚àí >1 . 0 : (m‚Äô =11) &(a ‚Äô =2) ;
40 [ ac ] (m=11) &(a =2) ‚àí >1 . 0 : (m‚Äô =11) &(a ‚Äô =2) ;
41 [ ac ] (m=12) &(a =0) ‚àí >1 . 0 : (m‚Äô =12) &(a ‚Äô =0) ;
5https://www.prismmodelchecker.org/download.php42 [ ac ] (m=12) &(a =1) ‚àí >1 . 0 : (m‚Äô =12) &(a ‚Äô =2) ;
43 [ ac ] (m=12) &(a =2) ‚àí >1 . 0 : (m‚Äô =12) &(a ‚Äô =2) ;
44 [ ac ] (m=13) &(a =0) ‚àí >1 . 0 : (m‚Äô =13) &(a ‚Äô =0) ;
45 [ ac ] (m=13) &(a =1) ‚àí >1 . 0 : (m‚Äô =13) &(a ‚Äô =2) ;
46 [ ac ] (m=13) &(a =2) ‚àí >1 . 0 : (m‚Äô =13) &(a ‚Äô =2) ;
47 [ ac ] (m=14) &(a =0) ‚àí >1 . 0 : (m‚Äô =14) &(a ‚Äô =0) ;
48 [ ac ] (m=14) &(a =1) ‚àí >1 . 0 : (m‚Äô =14) &(a ‚Äô =2) ;
49 [ ac ] (m=14) &(a =2) ‚àí >1 . 0 : (m‚Äô =14) &(a ‚Äô =2) ;
50 [ ac ] (m=15) &(a =0) ‚àí >1 . 0 : (m‚Äô =15) &(a ‚Äô =0) ;
51 [ ac ] (m=15) &(a =1) ‚àí >1 . 0 : (m‚Äô =15) &(a ‚Äô =2) ;
52 [ ac ] (m=15) &(a =2) ‚àí >1 . 0 : (m‚Äô =15) &(a ‚Äô =2) ;
53 [ ac ] (m=16) &(a =0) ‚àí >1 . 0 : (m‚Äô =16) &(a ‚Äô =0) ;
54 [ ac ] (m=16) &(a =1) ‚àí >1 . 0 : (m‚Äô =16) &(a ‚Äô =2) ;
55 [ ac ] (m=16) &(a =2) ‚àí >1 . 0 : (m‚Äô =16) &(a ‚Äô =2) ;
56 [ ac ] (m=17) &(a =0) ‚àí >1 . 0 : (m‚Äô =17) &(a ‚Äô =0) ;
57 [ ac ] (m=17) &(a =1) ‚àí >1 . 0 : (m‚Äô =17) &(a ‚Äô =2) ;
58 [ ac ] (m=17) &(a =2) ‚àí >1 . 0 : (m‚Äô =17) &(a ‚Äô =2) ;
59 [ ac ] (m=18) &(a =0) ‚àí >1 . 0 : (m‚Äô =18) &(a ‚Äô =0) ;
60 [ ac ] (m=18) &(a =1) ‚àí >1 . 0 : (m‚Äô =18) &(a ‚Äô =2) ;
61 [ ac ] (m=18) &(a =2) ‚àí >1 . 0 : (m‚Äô =18) &(a ‚Äô =2) ;
62 [ ac ] (m=19) &(a =0) ‚àí >1 . 0 : (m‚Äô =19) &(a ‚Äô =0) ;
63 [ ac ] (m=19) &(a =1) ‚àí >1 . 0 : (m‚Äô =19) &(a ‚Äô =2) ;
64 [ ac ] (m=19) &(a =2) ‚àí >1 . 0 : (m‚Äô =19) &(a ‚Äô =2) ;
65 [ ac ] (m=20) &(a =0) ‚àí >0 . 8 : (m‚Äô =21) &(a ‚Äô =0) + 0 . 2 : (m‚Äô =30) &(a ‚Äô =0) ;
66 [ ac ] (m=20) &(a =1) ‚àí >0 . 8 : (m‚Äô =21) &(a ‚Äô =2) + 0 . 2 : (m‚Äô =30) &(a ‚Äô =2) ;
67 [ ac ] (m=20) &(a =2) ‚àí >0 . 8 : (m‚Äô =21) &(a ‚Äô =2) + 0 . 2 : (m‚Äô =30) &(a ‚Äô =2) ;
68 [ ac ] (m=21) &(a =0) ‚àí >1 : (m‚Äô =22) &(a ‚Äô =0) ;
69 [ ac ] (m=21) &(a =1) ‚àí >1 . 0 : (m‚Äô =21) &(a ‚Äô =2) ;
70 [ ac ] (m=21) &(a =2) ‚àí >1 . 0 : (m‚Äô =21) &(a ‚Äô =2) ;
71 [ ac ] (m=22) &(a =0) ‚àí >1 : (m‚Äô =23) &(a ‚Äô =0) ;
72 [ ac ] (m=22) &(a =1) ‚àí >1 . 0 : (m‚Äô =22) &(a ‚Äô =2) ;
73 [ ac ] (m=22) &(a =2) ‚àí >1 . 0 : (m‚Äô =22) &(a ‚Äô =2) ;
74 [ ac ] (m=23) &(a =0) ‚àí >1 : (m‚Äô =24) &(a ‚Äô =0) ;
75 [ ac ] (m=23) &(a =1) ‚àí >1 . 0 : (m‚Äô =23) &(a ‚Äô =2) ;
76 [ ac ] (m=23) &(a =2) ‚àí >1 . 0 : (m‚Äô =23) &(a ‚Äô =2) ;
77 [ ac ] (m=24) &(a =0) ‚àí >1 : (m‚Äô =25) &(a ‚Äô =0) ;
78 [ ac ] (m=24) &(a =1) ‚àí >1 . 0 : (m‚Äô =24) &(a ‚Äô =2) ;
79 [ ac ] (m=24) &(a =2) ‚àí >1 . 0 : (m‚Äô =24) &(a ‚Äô =2) ;
80 [ ac ] (m=25) &(a =0) ‚àí >1 : (m‚Äô =26) &(a ‚Äô =0) ;
81 [ ac ] (m=25) &(a =1) ‚àí >1 . 0 : (m‚Äô =25) &(a ‚Äô =2) ;
82 [ ac ] (m=25) &(a =2) ‚àí >1 . 0 : (m‚Äô =25) &(a ‚Äô =2) ;
83 [ ac ] (m=26) &(a =0) ‚àí >1 : (m‚Äô =27) &(a ‚Äô =0) ;
84 [ ac ] (m=26) &(a =1) ‚àí >1 . 0 : (m‚Äô =26) &(a ‚Äô =2) ;
85 [ ac ] (m=26) &(a =2) ‚àí >1 . 0 : (m‚Äô =26) &(a ‚Äô =2) ;
86 [ ac ] (m=27) &(a =0) ‚àí >1 : (m‚Äô =28) &(a ‚Äô =0) ;
87 [ ac ] (m=27) &(a =1) ‚àí >1 . 0 : (m‚Äô =27) &(a ‚Äô =2) ;
88 [ ac ] (m=27) &(a =2) ‚àí >1 . 0 : (m‚Äô =27) &(a ‚Äô =2) ;
89 [ ac ] (m=28) &(a =0) ‚àí >1 : (m‚Äô =29) &(a ‚Äô =0) ;
90 [ ac ] (m=28) &(a =1) ‚àí >1 : (m‚Äô =27) &(a ‚Äô =2) ;
91 [ ac ] (m=28) &(a =2) ‚àí >1 : (m‚Äô =27) &(a ‚Äô =2) ;
92 [ ep ] (m=29) &(a =0) ‚àí >(m‚Äô =29) &(a ‚Äô =1) ;
93 [ ac ] (m=29) &(a =1) ‚àí >1 . 0 : (m‚Äô =29) &(a ‚Äô =1) ;
94 [ ac ] (m=29) &(a =2) ‚àí >1 . 0 : (m‚Äô =29) &(a ‚Äô =2) ;
95 [ ep ] (m=30) &(a =0) ‚àí >(m‚Äô =30) &(a ‚Äô =1) ;
96 [ ac ] (m=30) &(a =1) ‚àí >1 . 0 : (m‚Äô =30) &(a ‚Äô =2) ;
97 [ ac ] (m=30) &(a =2) ‚àí >1 . 0 : (m‚Äô =30) &(a ‚Äô =2) ;
98 [ ac ] (m=31) &(a =0) ‚àí >1 . 0 : (m‚Äô =31) &(a ‚Äô =0) ;
99 [ ac ] (m=31) &(a =1) ‚àí >1 . 0 : (m‚Äô =31) &(a ‚Äô =2) ;
100 [ ac ] (m=31) &(a =2) ‚àí >1 . 0 : (m‚Äô =31) &(a ‚Äô =2) ;
101 [ ac ] (m=32) &(a =0) ‚àí >1 . 0 : (m‚Äô =32) &(a ‚Äô =0) ;
102 [ ac ] (m=32) &(a =1) ‚àí >1 . 0 : (m‚Äô =32) &(a ‚Äô =2) ;
103 [ ac ] (m=32) &(a =2) ‚àí >1 . 0 : (m‚Äô =32) &(a ‚Äô =2) ;
104 [ ac ] (m=33) &(a =0) ‚àí >1 . 0 : (m‚Äô =33) &(a ‚Äô =0) ;
105 [ ac ] (m=33) &(a =1) ‚àí >1 . 0 : (m‚Äô =33) &(a ‚Äô =2) ;
106 [ ac ] (m=33) &(a =2) ‚àí >1 . 0 : (m‚Äô =33) &(a ‚Äô =2) ;107 [ ac ] (m=34) &(a =0) ‚àí >1 . 0 : (m‚Äô =34) &(a ‚Äô =0) ;
108 [ ac ] (m=34) &(a =1) ‚àí >1 . 0 : (m‚Äô =34) &(a ‚Äô =2) ;
109 [ ac ] (m=34) &(a =2) ‚àí >1 . 0 : (m‚Äô =34) &(a ‚Äô =2) ;
110 [ ac ] (m=35) &(a =0) ‚àí >1 . 0 : (m‚Äô =35) &(a ‚Äô =0) ;
111 [ ac ] (m=35) &(a =1) ‚àí >1 . 0 : (m‚Äô =35) &(a ‚Äô =2) ;
112 [ ac ] (m=35) &(a =2) ‚àí >1 . 0 : (m‚Äô =35) &(a ‚Äô =2) ;
113 [ ac ] (m=36) &(a =0) ‚àí >1 . 0 : (m‚Äô =36) &(a ‚Äô =0) ;
114 [ ac ] (m=36) &(a =1) ‚àí >1 . 0 : (m‚Äô =36) &(a ‚Äô =2) ;
115 [ ac ] (m=36) &(a =2) ‚àí >1 . 0 : (m‚Äô =36) &(a ‚Äô =2) ;
116 [ ac ] (m=37) &(a =0) ‚àí >1 . 0 : (m‚Äô =37) &(a ‚Äô =0) ;
117 [ ac ] (m=37) &(a =1) ‚àí >1 . 0 : (m‚Äô =37) &(a ‚Äô =2) ;
118 [ ac ] (m=37) &(a =2) ‚àí >1 . 0 : (m‚Äô =37) &(a ‚Äô =2) ;
119 [ ac ] (m=38) &(a =0) ‚àí >1 . 0 : (m‚Äô =38) &(a ‚Äô =0) ;
120 [ ac ] (m=38) &(a =1) ‚àí >1 . 0 : (m‚Äô =38) &(a ‚Äô =2) ;
121 [ ac ] (m=38) &(a =2) ‚àí >1 . 0 : (m‚Äô =38) &(a ‚Äô =2) ;
122 [ ac ] (m=39) &(a =0) ‚àí >1 . 0 : (m‚Äô =39) &(a ‚Äô =0) ;
123 [ ac ] (m=39) &(a =1) ‚àí >1 . 0 : (m‚Äô =39) &(a ‚Äô =2) ;
124 [ ac ] (m=39) &(a =2) ‚àí >1 . 0 : (m‚Äô =39) &(a ‚Äô =2) ;
125endmodule
126
127l a b e l ‚Äù a ‚Äù = (m=0) j(m=1) j(m=2) j(m=3) j(m=4) j(m=5) j(m=6) j(m=7) j(m=8) j(m=29) ;
128l a b e l ‚Äù c ‚Äù = (m=30) ;
Listing 1: The PRISM model of the induced Markov chain from the probabilistic gate MDP and its optimal policy.
B.5 Rabinizer
We use Rabinizer 4 [KÀáret¬¥ƒ±nsk¬¥yet al. , 2018 ]to transform LTL formulae into LDBAs. The tool can be downloaded on their
website6, and we use the ‚Äôltl2ldba‚Äô script with ‚Äô-e‚Äô and ‚Äô-d‚Äô options to construct the LDBA with -transitions from the input
LTL formulae.
6https://www7.in.tum.de/ kretinsk/rabinizer4.html